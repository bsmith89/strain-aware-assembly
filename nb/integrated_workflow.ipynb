{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "_os.chdir(_os.environ['PROJECT_ROOT'])\n",
    "print(_os.path.realpath(_os.path.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool as gt\n",
    "import graph_tool.draw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for constructing graphs\n",
    "def path_to_edgelist(path):\n",
    "    u = path[0]\n",
    "    edges = []\n",
    "    for v in path[1:]:\n",
    "        edges.append((u, v))\n",
    "        u = v\n",
    "    return edges\n",
    "\n",
    "def new_graph_from_merged_paths(paths, lengths, depths):\n",
    "    g = gt.Graph()\n",
    "    all_edges = []\n",
    "    for p in paths:\n",
    "        all_edges.extend(path_to_edgelist(p))\n",
    "    g.add_edge_list(set(all_edges))\n",
    "    g.vp['depth'] = g.new_vp('vector<float>')\n",
    "    g.vp.depth.set_2d_array(depths)\n",
    "    g.vp['length'] = g.new_vp('int', lengths)  \n",
    "    g.gp['nsample'] = g.new_gp('int', len(depths))\n",
    "    g.vp['sequence'] = g.new_vp('object', vals=[[k] for k in range(g.num_vertices())])\n",
    "    return g\n",
    "\n",
    "def get_depth_matrix(g, vs=None):\n",
    "    if not vs:\n",
    "        return g.vp.depth.get_2d_array(range(g.gp.nsample))\n",
    "    else:\n",
    "        return np.stack([g.vp.depth[i] for i in vs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "def estimate_flow(f0, d, sample_idx, eps=1e-2, maxiter=100):\n",
    "    loss_hist = [np.inf]\n",
    "    f = f0\n",
    "    for step_i in range(maxiter):\n",
    "        f_out = f\n",
    "        f_total_out = f_out.sum(1)\n",
    "        d_error_out = f_total_out - d\n",
    "        f_in = f_out.T\n",
    "        f_total_in = f_in.sum(1)\n",
    "        d_error_in = f_total_in - d\n",
    "        loss_hist.append(np.square(d_error_out).sum() + np.square(d_error_in).sum())\n",
    "        loss_ratio = (loss_hist[-2] - loss_hist[-1]) / loss_hist[-2]\n",
    "        # print(loss_ratio)\n",
    "        if loss_ratio < eps:\n",
    "            # print(loss_hist[-1], loss_ratio)\n",
    "            # print(step_i)\n",
    "            break\n",
    "        f_frac_out = (f_out.T * np.nan_to_num(1 / f_total_out, nan=0.0, posinf=1.0)).T\n",
    "        allocated_d_error_out = (d_error_out * f_frac_out.T).T\n",
    "        f_frac_in = (f_in.T * np.nan_to_num(1 / f_total_in, nan=0.0, posinf=1.0)).T\n",
    "        allocated_d_error_in = (d_error_in * f_frac_in.T).T\n",
    "        # TODO: Consider weighting the shift by length. A very short unitig shouldn't\n",
    "        # be equally influential as a very long one.\n",
    "        mean_allocated_d_error = (allocated_d_error_in.T + allocated_d_error_out) / 2\n",
    "        f = (f_out - mean_allocated_d_error)\n",
    "    else:\n",
    "        warn(f\"loss_ratio < eps ({eps}) not achieved in maxiter ({maxiter}) steps. Final loss_ratio={loss_ratio}. Final loss={loss_hist[-1]}.\")\n",
    "    return f\n",
    "\n",
    "\n",
    "def estimate_all_flows(g, eps=1e-3, maxiter=1000):\n",
    "    flows = []\n",
    "    for sample_idx in range(g.gp.nsample):\n",
    "        f0 = sp.sparse.csr_array(gt.spectral.adjacency(g))\n",
    "        d = g.vp.depth.get_2d_array([sample_idx])[0]\n",
    "        f = estimate_flow(f0, d, sample_idx=0, eps=eps, maxiter=maxiter)\n",
    "        flows.append(f)\n",
    "    return flows\n",
    "\n",
    "\n",
    "def mutate_add_flows(g, flows):\n",
    "    props = []\n",
    "    for sample_idx, f in enumerate(flows):\n",
    "        p = g.new_edge_property('float', val=0)\n",
    "        for i, j in g.get_edges():\n",
    "            p[g.edge(i, j)] = f[j, i]\n",
    "        props.append(p)\n",
    "    props = gt.group_vector_property(props)\n",
    "    g.ep['flow'] = props\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(g, **kwargs):\n",
    "    kwargs = dict(\n",
    "        vertex_fill_color=g.new_vertex_property('float', vals=np.linspace(0, 1, num=max(g.get_vertices()) + 1)),\n",
    "        vertex_text=g.vertex_index,\n",
    "        output_size=(300, 300),\n",
    "        ink_scale=0.8,\n",
    "    ) | kwargs\n",
    "    return gt.draw.graph_draw(g, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "paths = [\n",
    "    [0, 1, 2, 0],\n",
    "    [0, 3, 4, 0],\n",
    "    [0, 0],\n",
    "]\n",
    "\n",
    "nnodes = max(itertools.chain(*paths)) + 1\n",
    "\n",
    "depths = np.array([\n",
    "    [5, 5, 5, 0, 0],\n",
    "    [4, 0, 0, 4, 4],\n",
    "    [5, 1, 1, 1, 1],\n",
    "    [11, 2, 2, 2, 2],\n",
    "])\n",
    "nsamples = depths.shape[0]\n",
    "\n",
    "g0 = new_graph_from_merged_paths(\n",
    "    paths,\n",
    "    depths=depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "mutate_add_flows(g0, estimate_all_flows(g0))\n",
    "\n",
    "\n",
    "g0_pos = draw_graph(g0, vertex_text=g0.vertex_index)\n",
    "for i in range(g0.gp.nsample):\n",
    "    draw_graph(\n",
    "        g0,\n",
    "        pos=g0_pos,\n",
    "        vertex_text=gt.ungroup_vector_property(g0.vp.depth, pos=[i])[0],\n",
    "        edge_pen_width=gt.ungroup_vector_property(g0.ep.flow, pos=[i])[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_has_no_siblings(g):\n",
    "    \"Check whether upstream or downstream sibling edges exist for every edge.\"\n",
    "    vs = g.get_vertices()\n",
    "    v_in_degree = g.new_vertex_property('int', vals=g.get_in_degrees(vs))\n",
    "    v_out_degree = g.new_vertex_property('int', vals=g.get_out_degrees(vs))\n",
    "    e_num_in_siblings = gt.edge_endpoint_property(g, v_in_degree, 'target')\n",
    "    e_num_out_siblings = gt.edge_endpoint_property(g, v_out_degree, 'source')\n",
    "    e_has_no_sibling_edges = g.new_edge_property('bool', (e_num_in_siblings.a <= 1) & (e_num_out_siblings.a <= 1))\n",
    "    return e_has_no_sibling_edges\n",
    "\n",
    "def vertex_does_not_have_both_multiple_in_and_multiple_out(g):\n",
    "    vs = g.get_vertices()\n",
    "    return g.new_vertex_property('bool', vals=(\n",
    "        (g.get_in_degrees(vs) <= 1)\n",
    "        | (g.get_out_degrees(vs) <= 1)\n",
    "    ))\n",
    "\n",
    "\n",
    "def label_maximal_unitigs(g):\n",
    "    \"Assign unitig indices to vertices in maximal unitigs.\"\n",
    "    no_sibling_edges = edge_has_no_siblings(g)\n",
    "    # Since any vertex that has both multiple in _and_ multiple out\n",
    "    # edges cannot be part of a larger maximal unitig,\n",
    "    # we could filter out these vertices, at the same time as we\n",
    "    # are filtering out the edges with siblings.\n",
    "    # Potentially this would make the component labeling step\n",
    "    # much faster.\n",
    "    both_sides_branch = vertex_does_not_have_both_multiple_in_and_multiple_out(g)\n",
    "    # TODO: Double check, if this has any implications for the\n",
    "    # \"unitig-ness\" of its neighbors. I _think_\n",
    "    # if we mark edges with siblings before filtering out\n",
    "    # these nodes we should be good.\n",
    "    g_filt = gt.GraphView(\n",
    "        g,\n",
    "        efilt=no_sibling_edges,\n",
    "        vfilt=both_sides_branch,\n",
    "        directed=True\n",
    "    )\n",
    "    g_filt_undirected = gt.GraphView(g_filt, directed=False)\n",
    "    # Since we've filtered out the both_sides_branch vertices,\n",
    "    # the labels PropertyMap would include a bunch of the default value (0)\n",
    "    # for these. Instead, we set everything not labeled to -1, now a magic\n",
    "    # value for nodes definitely not in maximal unitigs.\n",
    "    labels = g.new_vertex_property('int', val=-1)\n",
    "    labels, counts = gt.topology.label_components(g_filt_undirected, vprop=labels)\n",
    "    return labels, counts, g_filt\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "\n",
    "def is_simple_cycle(g, vfilt):\n",
    "    g_filt = gt.GraphView(g, vfilt=vfilt)\n",
    "    vs = list(g_filt.iter_vertices())\n",
    "    path = list(gt.topology.all_paths(g_filt, vs[0], vs[0]))\n",
    "    if not len(path) == 1:\n",
    "        return False\n",
    "    elif set(path[0]) != set(vs):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def maximal_unitigs(g):\n",
    "    \"Generate maximal unitigs as lists of vertices\"\n",
    "    # (1a) Filter edges\n",
    "    # (1b) Label unitigs\n",
    "    labels, counts, g_filt = label_maximal_unitigs(g)\n",
    "    # (2) Find every node in the filtered graph without in-edges (these are the origins)\n",
    "    is_origin = g_filt.new_vp('bool', g_filt.get_in_degrees(g_filt.get_vertices()) == 0)\n",
    "    # (3) Find every node in the filtered graph without out-edges (these are the termina)\n",
    "    is_terminus = g_filt.new_vp('bool', g_filt.get_out_degrees(g_filt.get_vertices()) == 0)\n",
    "    # (4) Iter through the labels.\n",
    "    for i, c in enumerate(counts):\n",
    "        # (a) For each, select the subgraph for that unitig.\n",
    "        vfilt = (labels.a == i)\n",
    "        assert vfilt.sum() == c\n",
    "        subgraph = gt.GraphView(g_filt, vfilt=vfilt)\n",
    "        # (b) Identify the origin and terminus nodes in the path\n",
    "        vs = subgraph.get_vertices()\n",
    "        origin = gt.GraphView(subgraph, vfilt=is_origin).get_vertices()\n",
    "        terminus = gt.GraphView(subgraph, vfilt=is_terminus).get_vertices()\n",
    "        if (len(origin) == 0) and (len(terminus) == 0):\n",
    "            origin, terminus = vs[0], vs[0]\n",
    "        elif (len(origin) == 1) and (len(terminus) == 1):\n",
    "            origin, terminus = origin[0], terminus[0]\n",
    "        else:\n",
    "            raise AssertionError(\"If there are multiple origins or termina, then it's not a unitig.\")\n",
    "        # (c) Trace the route from the origin to the terminus (`graph_tool.topology.all_paths`)\n",
    "        unitig = list(gt.topology.all_paths(subgraph, origin, terminus))\n",
    "        if len(unitig) == 0:\n",
    "            continue  # Maximal unitig of length 1. No-op.\n",
    "        assert len(unitig) == 1, \"If there are multiple paths from origin to terminus, then it's not a unitig.\"\n",
    "        unitig = unitig[0]\n",
    "        # (d) Ask if it's a cycle.\n",
    "        is_cycle = is_simple_cycle(g, vfilt)\n",
    "        if is_cycle and (len(unitig) > 1) and (unitig[0] == unitig[-1]):\n",
    "            unitig = unitig[:-1]\n",
    "            assert len(set(unitig)) == len(unitig)\n",
    "        # (e) Yield the route and whether it's a cycle.\n",
    "        yield list(unitig), is_cycle\n",
    "        \n",
    "        \n",
    "def list_unitig_neighbors(g, vs):\n",
    "    \"The in and out neighbors of a unitig path.\"\n",
    "    all_ins = reduce(operator.add, map(lambda v: list(g.iter_in_neighbors(v)), vs))\n",
    "    all_outs = reduce(operator.add, map(lambda v: list(g.iter_out_neighbors(v)), vs))\n",
    "    return list(set(all_ins) - set(vs)), list(set(all_outs) - set(vs))\n",
    "\n",
    "def mutate_add_compressed_unitig_vertex(g, vs, is_cycle, drop_vs=False):\n",
    "    v = int(g.add_vertex())\n",
    "    nsample = g.gp.nsample\n",
    "    in_neighbors, out_neighbors = list_unitig_neighbors(g, vs)\n",
    "    g.add_edge_list((neighbor, v) for neighbor in in_neighbors)\n",
    "    g.add_edge_list((v, neighbor) for neighbor in out_neighbors)\n",
    "    g.vp.length.a[v] = g.vp.length.a[vs].sum()\n",
    "    g.vp.sequence[v] = reduce(operator.add, (g.vp.sequence[u] for u in vs), [])\n",
    "    g.vp.depth[v] = (\n",
    "        (\n",
    "            get_depth_matrix(g, vs)\n",
    "            * g.vp.length.a[vs]\n",
    "        ).sum(1) / g.vp.length.a[v]\n",
    "    )\n",
    "    assert np.allclose((get_depth_matrix(g, vs) * g.vp.length.a[vs]).sum(1), (get_depth_matrix(g, [v]) * g.vp.length.a[v]).sum(1))\n",
    "    if is_cycle:\n",
    "        g.add_edge(v, v)\n",
    "    for old_v in vs:\n",
    "        g.clear_vertex(old_v)\n",
    "    return g\n",
    "\n",
    "# FIXME: For some reason, when length-1 unitigs are compressed, the total depth of the graph increases... hmm...?\n",
    "# Could this be mostly due to self-looping nodes?\n",
    "# I think one possibility is that self-looping nodes are somehow being expanded out into ever-longer\n",
    "# chains of loops without correctly adjusting the depth.\n",
    "\n",
    "def mutate_compress_all_unitigs(g):\n",
    "    unitig_list = maximal_unitigs(g)\n",
    "    all_vs = []\n",
    "    for i, (vs, is_cycle) in enumerate(unitig_list):\n",
    "        # TODO: If len(vs) == 1, this is effectively a no-op and can be dropped.\n",
    "        mutate_add_compressed_unitig_vertex(g, vs, is_cycle)\n",
    "        all_vs.extend(vs)\n",
    "    g.remove_vertex(set(all_vs), fast=True)\n",
    "    # I think, but am not sure, that the number of nodes removed will always equal the number of edges removed.\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = mutate_compress_all_unitigs(g0.copy())\n",
    "mutate_add_flows(g1, estimate_all_flows(g1))\n",
    "\n",
    "g1_pos = draw_graph(g1, vertex_text=g1.vertex_index)\n",
    "for i in range(g1.gp.nsample):\n",
    "    draw_graph(\n",
    "        g1,\n",
    "        pos=g1_pos,\n",
    "        vertex_text=gt.ungroup_vector_property(g1.vp.depth, pos=[i])[0],\n",
    "        edge_pen_width=gt.ungroup_vector_property(g1.ep.flow, pos=[i])[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.decomposition import sparse_encode\n",
    "\n",
    "\n",
    "def compile_code_inference_inputs(g, v):\n",
    "    obs = []\n",
    "    i = 0\n",
    "    edge_idx = {}\n",
    "    for u in g.get_in_neighbors(v):\n",
    "        if (u, v) not in edge_idx:\n",
    "            edge_idx[(u, v)] = i\n",
    "            i += 1\n",
    "            obs.append(g.ep.flow[g.edge(u, v)])\n",
    "    for w in g.get_out_neighbors(v):\n",
    "        if (v, w) not in edge_idx:\n",
    "            edge_idx[(v, w)] = i\n",
    "            i += 1\n",
    "            obs.append(g.ep.flow[g.edge(v, w)])\n",
    "\n",
    "    dictionary = []\n",
    "    split_idx = {}\n",
    "    eye = np.eye(len(edge_idx))\n",
    "    i = 0\n",
    "    for u, w in product(g.get_in_neighbors(v), g.get_out_neighbors(v)):\n",
    "        split_idx[i] = (u, w)\n",
    "        i += 1\n",
    "        if u != w:\n",
    "            dictionary.append(eye[edge_idx[(u, v)]] + eye[edge_idx[(v, w)]])\n",
    "        else:\n",
    "            dictionary.append(eye[edge_idx[(u, v)]] + eye[edge_idx[(v, w)]])\n",
    "    # TODO: Consider how to weight these estimates by unitig length.\n",
    "    # A lar\n",
    "    return(np.stack(obs, axis=1), np.stack(dictionary), edge_idx, split_idx)\n",
    "\n",
    "\n",
    "def sparse_encode_gmp(X, dictionary, eps=0.1):\n",
    "    # Group Matching Pursuit (GMP)\n",
    "    # Inspired by https://arxiv.org/pdf/1812.10538.pdf\n",
    "    residual = X\n",
    "    dictionary0 = dictionary\n",
    "    dictionary = np.zeros_like(dictionary0)\n",
    "    atoms = []\n",
    "    loss0 = np.abs(X).sum()\n",
    "    loss_hist = []\n",
    "    encoding = np.zeros((X.shape[0], dictionary.shape[0]))\n",
    "    for _ in range(dictionary.shape[0]):\n",
    "        loss_hist.append(np.abs(residual).sum())\n",
    "        if loss_hist[-1] / loss0 < eps:\n",
    "            break\n",
    "        dot = residual @ dictionary0.T\n",
    "        atoms.append(dot.argmax() % dot.shape[1])\n",
    "        dictionary[atoms[-1]] = dictionary0[atoms[-1]]\n",
    "        encoding = sparse_encode(X, dictionary=dictionary, algorithm='lasso_lars', positive=True, alpha=0.0)\n",
    "        # TODO: How do I do it this way?\n",
    "        # encoding[:, atoms[-1]] += dot[:, atoms[-1]]\n",
    "        residual =  X - encoding @ dictionary\n",
    "    return encoding, loss_hist\n",
    "\n",
    "def print_split_details_from_sparse_encoding(g, v, eps=1e-2):\n",
    "    obs, dictionary, edge_idx, split_idx = compile_code_inference_inputs(g, v=v)\n",
    "    print(obs.round(3))\n",
    "    print(dictionary)\n",
    "    print(edge_idx)\n",
    "    split_depth, loss_hist = sparse_encode_gmp(obs, dictionary, eps=eps)\n",
    "    print(split_depth.round(3))\n",
    "    active_components = np.arange(split_depth.shape[1])[split_depth.sum(0) != 0]\n",
    "    for i in active_components:\n",
    "        print(i, split_idx[i], split_depth[:, i].round(2))\n",
    "        \n",
    "        \n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "\n",
    "Split = namedtuple('Split', ['u', 'v', 'w'])\n",
    "# TODO: Add functionality to split a node with only an in or only an out.\n",
    "# TODO: Use this functionality to keep \"residual\" nodes, that keep all of the depth unaccounted for\n",
    "# by the sparse coding.\n",
    "\n",
    "def all_local_paths_as_splits(g, v):\n",
    "    \"Generate all splits, the product of all in-edges crossed with all out-edges.\"\n",
    "    assert v < g.num_vertices(ignore_filter=True)\n",
    "    us = list(g.iter_in_neighbors(v))\n",
    "    ws = list(g.iter_out_neighbors(v))\n",
    "    num_splits = (len(us) * len(ws))\n",
    "    length = g.vp.length[v]\n",
    "    depth = get_depth_matrix(g, vs=[v])\n",
    "    for u, w in product(us, ws):\n",
    "        # NOTE: This (dummy) splitting function evenly distributes across all paths.\n",
    "        yield Split(u, v, w), length, depth / num_splits\n",
    "\n",
    "def splits_from_sparse_encoding(g, v, eps=1e-2):\n",
    "    obs, dictionary, edge_idx, split_idx = compile_code_inference_inputs(g, v=v)\n",
    "    split_depth, loss_hist = sparse_encode_gmp(obs, dictionary, eps=eps)\n",
    "    active_components = np.arange(split_depth.shape[1])[split_depth.sum(0) != 0]\n",
    "    for i in active_components:\n",
    "        u, w = split_idx[i]\n",
    "        yield Split(u, v, w), g.vp.length[v], split_depth[:, i]\n",
    "        \n",
    "list(splits_from_sparse_encoding(g1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits_from_sparse_encoding2(g, v):\n",
    "    # Group Matching Pursuit (GMP)\n",
    "    # Inspired by https://arxiv.org/pdf/1812.10538.pdf\n",
    "    in_neighbors = list(sorted(g.get_in_neighbors(v)))\n",
    "    in_neighbors_label = {k: v for k, v in enumerate(in_neighbors)}\n",
    "    in_neighbors_onehot = {k: v for k, v in zip(in_neighbors, np.eye(len(in_neighbors)))}\n",
    "\n",
    "    out_neighbors = list(sorted(g.get_out_neighbors(v)))\n",
    "    out_neighbors_label = {k: v for k, v in enumerate(out_neighbors)}\n",
    "    out_neighbors_onehot = {k: v for k, v in zip(out_neighbors, np.eye(len(out_neighbors)))}\n",
    "\n",
    "    in_neighbor_flow = []\n",
    "    for u in in_neighbors:\n",
    "        in_neighbor_flow.append(g.ep.flow[g.edge(u, v)])\n",
    "    in_neighbor_flow = np.stack(in_neighbor_flow)\n",
    "\n",
    "    out_neighbor_flow = []\n",
    "    for w in out_neighbors:\n",
    "        out_neighbor_flow.append(g.ep.flow[g.edge(v, w)])\n",
    "    out_neighbor_flow = np.stack(out_neighbor_flow)\n",
    "\n",
    "    in_neighbor_code = []\n",
    "    out_neighbor_code = []\n",
    "    split_idx = {}\n",
    "    for i, (u, w) in enumerate(product(in_neighbors, out_neighbors)):\n",
    "        in_neighbor_code.append(in_neighbors_onehot[u])\n",
    "        out_neighbor_code.append(out_neighbors_onehot[w])\n",
    "        split_idx[i] = (u, w)\n",
    "    in_neighbor_code = np.stack(in_neighbor_code)\n",
    "    out_neighbor_code = np.stack(out_neighbor_code)\n",
    "\n",
    "    obs = np.concatenate([in_neighbor_flow, out_neighbor_flow]).T\n",
    "    code = np.concatenate([in_neighbor_code, out_neighbor_code], axis=1)\n",
    "\n",
    "    resid = obs\n",
    "    atoms = []\n",
    "    dictionary = np.zeros_like(code)\n",
    "\n",
    "    for _ in range(code.shape[0]):\n",
    "        loss = np.abs(resid).sum()\n",
    "        # print('-----', loss)\n",
    "        dot = resid @ code.T\n",
    "        # print(dot.round(2))\n",
    "        dot_sum = dot.sum(0)\n",
    "        # print(dot.sum(0).round(5))\n",
    "        if dot_sum.max() <= 1e-5:\n",
    "            # print(\"No atoms to add.\")\n",
    "            break\n",
    "        atoms.append(dot.sum(0).argmax())\n",
    "        # print(atoms)\n",
    "        dictionary[atoms[-1]] = code[atoms[-1]]\n",
    "        # print(dictionary)\n",
    "        # encoding = sparse_encode(obs, dictionary=dictionary, algorithm='lasso_lars', positive=True, alpha=0.)\n",
    "        encoding, _, _ = non_negative_factorization(obs, n_components=dictionary.shape[0], H=dictionary, update_H=False)\n",
    "        # print(encoding.round(2))\n",
    "        resid = obs - encoding @ code\n",
    "        \n",
    "    active_components = np.arange(encoding.shape[1])[encoding.sum(0) != 0]\n",
    "    for i in active_components:\n",
    "        u, w = split_idx[i]\n",
    "        yield Split(u, v, w), g.vp.length[v], encoding[:, i]\n",
    "\n",
    "list(splits_from_sparse_encoding2(g1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tables_from_splits(split_list, start_idx):\n",
    "    \"\"\"Generate edges to and from new, split vertices.\n",
    "    \n",
    "    Note that if splits from adjacent parents are not\n",
    "    reciprocated, no new edge is produced.\n",
    "    \n",
    "    \"\"\"\n",
    "    split_idx = {}\n",
    "    upstream = defaultdict(list)\n",
    "    downstream = defaultdict(list)\n",
    "    length = []\n",
    "    depth = []\n",
    "    for idx, (split, l, d) in enumerate(split_list, start=start_idx):\n",
    "        u, v, w = split\n",
    "        split_idx[split] = idx\n",
    "        upstream[(v, w)].append(split)\n",
    "        downstream[(u, v)].append(split)\n",
    "        depth.append(d)\n",
    "        length.append(l)\n",
    "    return split_idx, upstream, downstream, np.array(length), np.array(depth)\n",
    "        \n",
    "        \n",
    "def new_edges_from_splits(split_list, split_idx, upstream, downstream, start_idx):\n",
    "    for v, (split, _, _) in enumerate(split_list, start=start_idx):\n",
    "        u_old, v_old, w_old = split\n",
    "        v = split_idx[split]\n",
    "        \n",
    "        # Upstream edges\n",
    "        if u_old is not None:\n",
    "            yield (u_old, v)\n",
    "        for upstream_split in upstream[(u_old, v_old)]:\n",
    "            u = split_idx[upstream_split]\n",
    "            if u is not None:\n",
    "                yield (u, v)\n",
    "            \n",
    "        # Downstream edges\n",
    "        if w_old is not None:\n",
    "            yield (v, w_old)\n",
    "        for downstream_split in downstream[(v_old, w_old)]:\n",
    "            w = split_idx[downstream_split]\n",
    "            if w is not None:\n",
    "                yield (v, w)\n",
    "            \n",
    "            \n",
    "def mutate_apply_splits(g, split_list):\n",
    "    \"\"\"Add edges and drop any parent vertices that were split.\n",
    "    \n",
    "    \"\"\"\n",
    "    start_idx = g.num_vertices(ignore_filter=True)\n",
    "    split_idx, upstream, downstream, lengths, depths = (\n",
    "        build_tables_from_splits(split_list, start_idx=start_idx)\n",
    "    )\n",
    "    edges_to_add = list(set(new_edges_from_splits(\n",
    "        split_list, split_idx, upstream, downstream, start_idx\n",
    "    )))\n",
    "    \n",
    "    # NOTE: Without adding vertices before edges I can get an IndexError\n",
    "    # running `g.vp.length.a[np.arange(len(lengths)) + start_idx]`.\n",
    "    # I believe this is because one or more split nodes\n",
    "    # are without any new edges, and therefore these nodes don't\n",
    "    # get added implicitly by `g.add_edge_list`.\n",
    "    # When these accidentally hidden nodes are the highest valued\n",
    "    # ones, they also don't get implicitly added due to their index.\n",
    "    # The result is that I'm missing nodes that should actually exist.\n",
    "    g.add_vertex(n=max(split_idx.values()) - max(g.get_vertices()))\n",
    "    assert max(g.get_vertices()) == max(split_idx.values()) \n",
    "    g.add_edge_list(set(edges_to_add))\n",
    "    \n",
    "    g.vp.length.a[np.arange(len(lengths)) + start_idx] = lengths\n",
    "    new_depth = get_depth_matrix(g)\n",
    "    new_depth[:, np.arange(len(depths)) + start_idx] = depths.T\n",
    "    g.vp.depth.set_2d_array(new_depth)\n",
    "    for split, _, _ in split_list:\n",
    "        g.vp.sequence[split_idx[split]] = g.vp.sequence[split.v]\n",
    "    vertices_to_drop = set(split.v for (split, _, _) in split_list)\n",
    "    g.remove_vertex(vertices_to_drop, fast=True)\n",
    "    return g\n",
    "\n",
    "\n",
    "def splits_for_all_vertices(g, split_func):\n",
    "    for v in g.vertices():\n",
    "        if (v.in_degree() < 2) and (v.out_degree() < 2):\n",
    "            continue\n",
    "        else:\n",
    "            yield from split_func(g, v)\n",
    "        \n",
    "def mutate_split_all_nodes(g, split_func):\n",
    "    # TODO: Vertices with <= 1 local path will be split into just\n",
    "    # themselves pointing trivially at their neighbors.\n",
    "    # These can be dropped as they are effectively a no-op.\n",
    "    split_list = list(splits_for_all_vertices(g, split_func))\n",
    "    if len(split_list) > 0:\n",
    "        g = mutate_apply_splits(g, split_list=split_list)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_depth_length(g):\n",
    "    return (get_depth_matrix(g) * g.vp.length.a).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(g1, pos=g1_pos, vertex_text=g1.vertex_index)\n",
    "\n",
    "for i in range(g1.gp.nsample):\n",
    "    draw_graph(\n",
    "        g1,\n",
    "        pos=g1_pos,\n",
    "        vertex_text=gt.ungroup_vector_property(g1.vp.depth, pos=[i])[0],\n",
    "        edge_pen_width=gt.ungroup_vector_property(g1.ep.flow, pos=[i])[0]\n",
    "    )\n",
    "    \n",
    "print(g1.vp.length.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = mutate_apply_splits(g1.copy(), list(all_local_paths_as_splits(g1, 0)))\n",
    "mutate_add_flows(g2, estimate_all_flows(g2))\n",
    "\n",
    "g2_pos = draw_graph(g2, vertex_text=g2.vertex_index)\n",
    "\n",
    "for i in range(g2.gp.nsample):\n",
    "    draw_graph(\n",
    "        g2,\n",
    "        pos=g2_pos,\n",
    "        # vertex_text=gt.ungroup_vector_property(g2.vp.depth, pos=[i])[0],\n",
    "        edge_pen_width=gt.ungroup_vector_property(g2.ep.flow, pos=[i])[0]\n",
    "    )\n",
    "    \n",
    "print(\n",
    "    (get_depth_matrix(g1) * g1.vp.length.a).sum(),\n",
    "    (get_depth_matrix(g2) * g2.vp.length.a).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(splits_from_sparse_encoding2(g1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = mutate_apply_splits(g1.copy(), list(splits_from_sparse_encoding2(g1, 0)))\n",
    "mutate_add_flows(g3, estimate_all_flows(g3))\n",
    "\n",
    "g3_pos = draw_graph(g3, vertex_text=g3.vertex_index)\n",
    "\n",
    "for i in range(g3.gp.nsample):\n",
    "    draw_graph(\n",
    "        g3,\n",
    "        pos=g3_pos,\n",
    "        # vertex_text=gt.ungroup_vector_property(g3.vp.depth, pos=[i])[0],\n",
    "        edge_pen_width=gt.ungroup_vector_property(g3.ep.flow, pos=[i])[0]\n",
    "    )\n",
    "\n",
    "print(\n",
    "    total_depth_length(g2),\n",
    "    total_depth_length(g3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g4 = mutate_compress_all_unitigs(g3.copy())\n",
    "mutate_add_flows(g4, estimate_all_flows(g4))\n",
    "g4_pos = draw_graph(g4, vertex_text=g4.vertex_index)\n",
    "\n",
    "# for i in range(g4.gp.nsample):\n",
    "#     draw_graph(\n",
    "#         g4,\n",
    "#         pos=g4_pos,\n",
    "#         # vertex_text=gt.ungroup_vector_property(g3.vp.depth, pos=[i])[0],\n",
    "#         edge_pen_width=gt.ungroup_vector_property(g4.ep.flow, pos=[i])[0]\n",
    "#     )\n",
    "    \n",
    "# FIXME: Why isn't total depth (mostly) invariant?\n",
    "# Oddly: This happens much more during unitig compression than node splitting.\n",
    "print(\n",
    "    total_depth_length(g3),\n",
    "    total_depth_length(g4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "paths = [\n",
    "    [0, 1, 2, 3, 4],\n",
    "    [2, 2],\n",
    "]\n",
    "\n",
    "nnodes = max(itertools.chain(*paths)) + 1\n",
    "\n",
    "depths = np.array([\n",
    "    [5, 5, 5, 5, 5],\n",
    "    [1, 1, 2, 1, 1],\n",
    "])\n",
    "nsamples = depths.shape[0]\n",
    "\n",
    "g5 = new_graph_from_merged_paths(\n",
    "    paths,\n",
    "    depths=depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "mutate_add_flows(g5, estimate_all_flows(g5))\n",
    "\n",
    "\n",
    "g5_pos = draw_graph(g5, vertex_text=g5.vertex_index)\n",
    "for i in range(g5.gp.nsample):\n",
    "    draw_graph(\n",
    "        g5,\n",
    "        pos=g5_pos,\n",
    "        vertex_text=gt.ungroup_vector_property(g5.vp.depth, pos=[i])[0],\n",
    "        edge_pen_width=gt.ungroup_vector_property(g5.ep.flow, pos=[i])[0]\n",
    "    )\n",
    "    \n",
    "print(total_depth_length(g5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g6 = mutate_compress_all_unitigs(g5.copy())\n",
    "mutate_add_flows(g6, estimate_all_flows(g6))\n",
    "g6_pos = draw_graph(g6, vertex_text=g6.vertex_index)\n",
    "for i in range(g6.gp.nsample):\n",
    "    draw_graph(\n",
    "        g6,\n",
    "        pos=g6_pos,\n",
    "        vertex_text=gt.ungroup_vector_property(g6.vp.depth, pos=[i])[0],\n",
    "        edge_pen_width=gt.ungroup_vector_property(g6.ep.flow, pos=[i])[0]\n",
    "    )\n",
    "    \n",
    "print(total_depth_length(g6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(g1, vertex_text=g1.vertex_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g7 = mutate_split_all_nodes(g1.copy(), splits_from_sparse_encoding2)\n",
    "mutate_add_flows(g7, estimate_all_flows(g7))\n",
    "\n",
    "g7_pos = draw_graph(g7, vertex_text=g7.vertex_index)\n",
    "\n",
    "# for i in range(g7.gp.nsample):\n",
    "#     draw_graph(\n",
    "#         g7,\n",
    "#         pos=g7_pos,\n",
    "#         # vertex_text=gt.ungroup_vector_property(g7.vp.depth, pos=[i])[0],\n",
    "#         edge_pen_width=gt.ungroup_vector_property(g7.ep.flow, pos=[i])[0]\n",
    "#     )\n",
    "\n",
    "print(\n",
    "    total_depth_length(g1),\n",
    "    total_depth_length(g7),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "paths = dict(\n",
    "    a=list(range(1000)),\n",
    "    b=np.random.choice(range(1000), size=10)\n",
    ")\n",
    "depths = dict(\n",
    "    a=[1, 10, 1, 15, 0],\n",
    "    b=[0.1, 0.1, 0.5, 0.1, 0.1],\n",
    ")\n",
    "\n",
    "nnodes = max(itertools.chain(*paths.values())) + 1\n",
    "nsamples = 5\n",
    "\n",
    "_depths = np.zeros((nsamples, nnodes))\n",
    "for p in paths:\n",
    "    _depths[:, paths[p]] = np.outer(depths[p], np.ones(len(paths[p])))\n",
    "    \n",
    "_depths\n",
    "\n",
    "g8 = new_graph_from_merged_paths(\n",
    "    paths.values(),\n",
    "    depths=_depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "mutate_compress_all_unitigs(g8)\n",
    "\n",
    "for i in range(4):\n",
    "    # draw_graph(g8)\n",
    "    # print(i, g8)\n",
    "    mutate_add_flows(g8, estimate_all_flows(g8))\n",
    "    mutate_split_all_nodes(g8, splits_from_sparse_encoding2)\n",
    "    mutate_compress_all_unitigs(g8)\n",
    "\n",
    "plt.hist((g8.degree_property_map('in').a * g8.degree_property_map('out').a))\n",
    "print(g8)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame(dict(\n",
    "    total_depth=get_depth_matrix(g8).sum(0),\n",
    "    length=g8.vp.length.a,\n",
    ")).assign(total_depth_length=lambda x: x.total_depth * x.length).sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph building\n",
    "\n",
    "def path_to_edgelist(path):\n",
    "    u = path[0]\n",
    "    edges = []\n",
    "    for v in path[1:]:\n",
    "        edges.append((u, v))\n",
    "        u = v\n",
    "    return edges\n",
    "\n",
    "def new_graph_from_merged_paths(paths, lengths, depths):\n",
    "    g = gt.Graph()\n",
    "    all_edges = []\n",
    "    for p in paths:\n",
    "        all_edges.extend(path_to_edgelist(p))\n",
    "    g.add_edge_list(set(all_edges))\n",
    "    g.vp['depth'] = g.new_vp('vector<float>')\n",
    "    g.vp.depth.set_2d_array(depths)\n",
    "    g.vp['length'] = g.new_vp('int', lengths)  \n",
    "    g.gp['nsample'] = g.new_gp('int', len(depths))\n",
    "    g.vp['sequence'] = g.new_vp('object', vals=[[k] for k in range(g.num_vertices())])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(sp.spatial.distance.squareform(sp.spatial.distance.pdist(np.array([g8.vp.sequence[4]]).T, metric=lambda x, y: np.abs(x - y))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(reversed(sorted(enumerate((get_depth_matrix(g8).sum(0) * g8.vp.length.a)), key=lambda x: x[1])))[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g8.vp.depth[187]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g8.vp.length[187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    [0, 1, 2, 0],\n",
    "    [0, 3, 4, 0],\n",
    "    [0, 0],\n",
    "]\n",
    "\n",
    "nnodes = max(itertools.chain(*paths)) + 1\n",
    "\n",
    "depths = np.array([\n",
    "    [5, 5, 5, 0, 0],\n",
    "    [4, 0, 0, 4, 4],\n",
    "    [5, 1, 1, 1, 1],\n",
    "    [11, 2, 2, 2, 2],\n",
    "])\n",
    "nsamples = depths.shape[0]\n",
    "\n",
    "g = new_graph_from_merged_paths(\n",
    "    paths,\n",
    "    depths=depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "mutate_add_flows(g, estimate_all_flows(g, eps=1e-4, maxiter=1000))\n",
    "v = 0\n",
    "\n",
    "def splits_from_sparse_encoding3(g, v, eps=1e-2):\n",
    "    # Group Matching Pursuit (GMP)\n",
    "    # Inspired by https://arxiv.org/pdf/1812.10538.pdf\n",
    "    in_neighbors = list(sorted(g.get_in_neighbors(v)))\n",
    "    num_in_neighbors = len(in_neighbors)\n",
    "    in_neighbors_label = {k: v for k, v in enumerate(in_neighbors)}\n",
    "    in_neighbors_onehot = {k: v for k, v in zip(in_neighbors, np.eye(num_in_neighbors))}\n",
    "    in_neighbors_onehot[None] = np.zeros(num_in_neighbors)\n",
    "\n",
    "    out_neighbors = list(sorted(g.get_out_neighbors(v)))\n",
    "    num_out_neighbors = len(out_neighbors)\n",
    "    out_neighbors_label = {k: v for k, v in enumerate(out_neighbors)}\n",
    "    out_neighbors_onehot = {k: v for k, v in zip(out_neighbors, np.eye(num_out_neighbors))}\n",
    "    out_neighbors_onehot[None] = np.zeros(num_out_neighbors)\n",
    "\n",
    "    in_neighbor_flow = []\n",
    "    for u in in_neighbors:\n",
    "        in_neighbor_flow.append(g.ep.flow[g.edge(u, v)])\n",
    "    # in_neighbor_flow = np.stack(in_neighbor_flow)\n",
    "\n",
    "    out_neighbor_flow = []\n",
    "    for w in out_neighbors:\n",
    "        out_neighbor_flow.append(g.ep.flow[g.edge(v, w)])\n",
    "    # out_neighbor_flow = np.stack(out_neighbor_flow)\n",
    "\n",
    "    in_neighbor_code = []\n",
    "    out_neighbor_code = []\n",
    "    split_idx = {}\n",
    "    for i, (u, w) in enumerate(product(in_neighbors + [None], out_neighbors + [None])):\n",
    "        if (u, w) == (None, None):\n",
    "            continue\n",
    "            # NOTE: I DON'T include a node-only atom,\n",
    "            # because I'll return this always and I don't want\n",
    "            # to double-count.\n",
    "        in_neighbor_code.append(in_neighbors_onehot[u])\n",
    "        out_neighbor_code.append(out_neighbors_onehot[w])\n",
    "        split_idx[i] = (u, w)\n",
    "    in_neighbor_code = np.stack(in_neighbor_code)\n",
    "    out_neighbor_code = np.stack(out_neighbor_code)\n",
    "\n",
    "    depth_row = g.vp.depth[v].a\n",
    "    obs = np.stack(in_neighbor_flow + [depth_row] + out_neighbor_flow).T\n",
    "    code = np.concatenate([\n",
    "        in_neighbor_code,\n",
    "        np.ones((in_neighbor_code.shape[0], 1)),\n",
    "        out_neighbor_code\n",
    "    ], axis=1)\n",
    "    # print(obs.round(2))\n",
    "    # print(code)\n",
    "\n",
    "    resid = obs\n",
    "    atoms = []\n",
    "    dictionary = np.zeros_like(code)\n",
    "    encoding = np.zeros((obs.shape[0], dictionary.shape[0]))\n",
    "    for _ in range(code.shape[0]):\n",
    "        loss = np.abs(resid).sum()\n",
    "        # print('-----', loss)\n",
    "        dot = resid @ code.T\n",
    "        # print(dot.round(2))\n",
    "        next_atom = dot.argmax() % code.shape[0]\n",
    "        # TODO: Decide if I want to add the atom with the single element\n",
    "        # or the atom with the largest _summed_ dot-product.\n",
    "        if dot[:, next_atom].sum() <= eps:\n",
    "            # print(\"No atoms to add.\", dot.round(5))\n",
    "            break\n",
    "        atoms.append(next_atom)\n",
    "        # print(atoms)\n",
    "        dictionary[atoms[-1]] = code[atoms[-1]]\n",
    "        # print(dictionary)\n",
    "        # encoding = sparse_encode(obs, dictionary=dictionary, algorithm='lasso_lars', positive=True, alpha=0.)\n",
    "        encoding, _, _ = non_negative_factorization(obs, n_components=dictionary.shape[0], H=dictionary, update_H=False, alpha_W=0)\n",
    "        # print(encoding.round(2))\n",
    "        resid = obs - encoding @ code\n",
    "        # print('-----', split_idx[atoms[-1]], dot_sum.max(), np.abs(resid).sum())\n",
    "        # print(resid.round(2))\n",
    "    # print(depth_row, encoding.sum(1))\n",
    "    active_components = np.arange(encoding.shape[1])[encoding.sum(0) != 0]\n",
    "    for i in atoms:\n",
    "        u, w = split_idx[i]\n",
    "        yield Split(u, v, w), g.vp.length[v], encoding[:, i]\n",
    "    remaining_depth = g.vp.depth[v] - encoding.sum(1)\n",
    "    if not np.allclose(remaining_depth, 0):\n",
    "        yield Split(None, v, None), g.vp.length[v], g.vp.depth[v] - encoding.sum(1)\n",
    "\n",
    "list(splits_from_sparse_encoding3(g, 0, eps=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "strain_length = 100\n",
    "num_mutations = 50\n",
    "strainA_path = np.arange(strain_length)\n",
    "\n",
    "strainB_mutations = np.random.choice(strainA_path, size=num_mutations, replace=False)\n",
    "strainB_path = strainA_path.copy()\n",
    "strainB_path[strainB_mutations] = np.arange(strain_length, strain_length + num_mutations)\n",
    "\n",
    "strainC_mutations = np.random.choice(strainA_path, size=num_mutations, replace=False)\n",
    "strainC_path = strainA_path.copy()\n",
    "inversion_start = strain_length // 4\n",
    "inversion_end = 3 * strain_length // 4\n",
    "strainC_path[inversion_start : inversion_end] = strainC_path[inversion_end : inversion_start : -1]\n",
    "strainC_path_without_mutations = strainC_path.copy()\n",
    "strainC_path[strainC_mutations] = np.arange(strain_length + num_mutations, strain_length + 2 * num_mutations)\n",
    "\n",
    "paths = dict(\n",
    "    a=strainA_path,\n",
    "    b=strainB_path,\n",
    "    c=strainC_path,\n",
    ")\n",
    "depths = dict(\n",
    "    a=[10, 1, 1],\n",
    "    b=[1, 10, 1],\n",
    "    c=[1, 1, 10],\n",
    ")\n",
    "\n",
    "nnodes = max(itertools.chain(*paths.values())) + 1\n",
    "nsamples = 3\n",
    "\n",
    "_depths = np.zeros((nsamples, nnodes))\n",
    "for p in paths:\n",
    "    _depths[:, paths[p]] += np.outer(depths[p], np.ones(len(paths[p])))\n",
    "    \n",
    "_depths\n",
    "\n",
    "g8 = new_graph_from_merged_paths(\n",
    "    paths.values(),\n",
    "    depths=_depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "draw_graph(g8)\n",
    "print(g8)\n",
    "mutate_compress_all_unitigs(g8)\n",
    "\n",
    "for i in range(3):\n",
    "    draw_graph(g8)\n",
    "    print(g8)\n",
    "    # print(i, g8)\n",
    "    mutate_add_flows(g8, estimate_all_flows(g8, maxiter=1000, eps=1e-3))\n",
    "    # print(total_depth_length(g8))\n",
    "    mutate_split_all_nodes(g8, partial(splits_from_sparse_encoding3, eps=5))\n",
    "    # print(total_depth_length(g8))\n",
    "    mutate_compress_all_unitigs(g8)\n",
    "    # print(total_depth_length(g8))\n",
    "draw_graph(g8)\n",
    "print(g8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reversed(sorted(enumerate(g8.vp.length.a), key=lambda x: x[1])))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 98\n",
    "s = np.array(g8.vp.sequence[i])\n",
    "g8.vp.length[i], g8.vp.depth[i], dict(zip(*np.unique(s[1:] - s[:-1], return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "strain_length = 1000\n",
    "num_mutations = strain_length // 2\n",
    "ancestral_path = np.arange(strain_length)\n",
    "\n",
    "strainA_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainA_path = ancestral_path.copy()\n",
    "strainA_path[strainA_mutations] = np.arange(strain_length + 0 * num_mutations, strain_length + 1 * num_mutations)\n",
    "\n",
    "strainB_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainB_path = ancestral_path.copy()\n",
    "strainB_path[strainB_mutations] = np.arange(strain_length + 1 * num_mutations, strain_length + 2 * num_mutations)\n",
    "\n",
    "strainC_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainC_path = ancestral_path.copy()\n",
    "inversion_start = strain_length // 4\n",
    "inversion_end = 3 * strain_length // 4\n",
    "strainC_path[inversion_start : inversion_end] = strainC_path[inversion_end : inversion_start : -1]\n",
    "strainC_path_without_mutations = strainC_path.copy()\n",
    "strainC_path[strainC_mutations] = np.arange(strain_length + 2 * num_mutations, strain_length + 3 * num_mutations)\n",
    "\n",
    "paths = dict(\n",
    "    a=strainA_path,\n",
    "    b=strainB_path,\n",
    "    c=strainC_path,\n",
    "    x=ancestral_path,\n",
    ")\n",
    "mean_depths = dict(\n",
    "    a=[ 100,  0.1,  0.1 ],\n",
    "    b=[ 0.1,  10 ,  0.1 ],\n",
    "    c=[ 0.1,  0.1,  5   ],\n",
    "    x=[ 0  ,  0  ,  0   ],\n",
    ")\n",
    "\n",
    "\n",
    "nnodes = max(itertools.chain(*paths.values())) + 1\n",
    "nsamples = 3\n",
    "\n",
    "expected_depths = np.zeros((nsamples, nnodes))\n",
    "for p in paths:\n",
    "    expected_depths[:, paths[p]] += np.outer(mean_depths[p], np.ones(len(paths[p])))\n",
    "    \n",
    "_depths = sp.stats.poisson(mu=expected_depths * 10).rvs()\n",
    "\n",
    "g9 = new_graph_from_merged_paths(\n",
    "    paths.values(),\n",
    "    depths=_depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "draw_graph(g9)\n",
    "print(g9)\n",
    "mutate_compress_all_unitigs(g9)\n",
    "\n",
    "for i in range(3):\n",
    "    draw_graph(g9)\n",
    "    print(g9)\n",
    "    # print(i, g9)\n",
    "    mutate_add_flows(g9, estimate_all_flows(g9, maxiter=1000, eps=1e-3))\n",
    "    # print(total_depth_length(g9))\n",
    "    mutate_split_all_nodes(g9, partial(splits_from_sparse_encoding3, eps=50))\n",
    "    # print(total_depth_length(g9))\n",
    "    mutate_compress_all_unitigs(g9)\n",
    "    # print(total_depth_length(g9))\n",
    "draw_graph(g9)\n",
    "print(g9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = pd.DataFrame(dict(\n",
    "    in_degree=g9.degree_property_map('in').a,\n",
    "    out_degree=g9.degree_property_map('out').a,\n",
    "    length=g9.vp.length.a,\n",
    "))\n",
    "vp = vp.join(pd.DataFrame(get_depth_matrix(g9).T, columns=['a', 'b', 'c']))\n",
    "vp = vp.join(vp[['a', 'b', 'c']].multiply(vp.length, axis=0), rsuffix='_x_len')\n",
    "vp.sort_values('a_x_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-200, 1200, num=100)\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, sharex=True, sharey=True, figsize=(15, 5))\n",
    "for (label, predicate), col in zip({\n",
    "    '=0': (vp.in_degree + vp.out_degree) == 0,\n",
    "    '=1': (vp.in_degree + vp.out_degree) == 1,\n",
    "    '=2': (vp.in_degree + vp.out_degree) == 2,\n",
    "    '>2': (vp.in_degree + vp.out_degree) > 2,\n",
    "}.items(), axs.T):\n",
    "    for sample, ax in zip(['a', 'b', 'c'], col):\n",
    "        ax.hist(vp[predicate][sample], bins=bins, alpha=0.5, label=label)\n",
    "        ax.legend(loc='upper right')\n",
    "axs[0,0].set_yscale('symlog')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_seqs = vp.length.sort_values(ascending=False).head(10).index\n",
    "vp.loc[longest_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = longest_seqs\n",
    "\n",
    "fig, axs = plt.subplots(len(query_list), len(ref_list), figsize=(3 * len(ref_list), 3 * len(query_list)), sharex=True, sharey=True)\n",
    "axs = axs.reshape((len(query_list), len(ref_list)))\n",
    "\n",
    "for (i, query_idx), (j, ref_idx), in product(enumerate(query_list), enumerate(ref_list)):\n",
    "    ax = axs[i, j]\n",
    "    ref = paths[ref_idx]\n",
    "    query = g9.vp.sequence[query_idx]\n",
    "    ref = np.asanyarray(ref)\n",
    "    query = np.asanyarray(query)\n",
    "    length = max(len(ref), len(query))\n",
    "    query = np.pad(query, (0, length - len(query)), constant_values=-1)\n",
    "    ref = np.pad(ref, (0, length - len(ref)), constant_values=-1)\n",
    "    dotplot = sp.spatial.distance.cdist(query.reshape((-1, 1)), ref.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(dotplot.T), marker='o', s=1)\n",
    "    # ax.imshow(dotplot, cmap='Greys', origin='lower')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.annotate(int(vp.loc[query_idx][ref_idx].round()), xy=(0.1, 0.9), xycoords='axes fraction', va='top')\n",
    "        \n",
    "for ref_idx, bottom_row_ax in zip(ref_list, axs[-1,:]):\n",
    "    bottom_row_ax.set_xlabel(ref_idx)\n",
    "    \n",
    "for query_idx, left_column_ax in zip(query_list, axs[:, 0]):\n",
    "    left_column_ax.set_ylabel(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "strain_length = 1000\n",
    "num_mutations = strain_length // 2\n",
    "ancestral_path = np.arange(strain_length)\n",
    "\n",
    "strainA_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainA_path = ancestral_path.copy()\n",
    "strainA_path[strainA_mutations] = np.arange(strain_length + 0 * num_mutations, strain_length + 1 * num_mutations)\n",
    "\n",
    "strainB_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainB_path = ancestral_path.copy()\n",
    "strainB_path[strainB_mutations] = np.arange(strain_length + 1 * num_mutations, strain_length + 2 * num_mutations)\n",
    "\n",
    "strainC_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainC_path = ancestral_path.copy()\n",
    "inversion_start = strain_length // 4\n",
    "inversion_end = 3 * strain_length // 4\n",
    "strainC_path[inversion_start : inversion_end] = strainC_path[inversion_end : inversion_start : -1]\n",
    "strainC_path_without_mutations = strainC_path.copy()\n",
    "strainC_path[strainC_mutations] = np.arange(strain_length + 2 * num_mutations, strain_length + 3 * num_mutations)\n",
    "\n",
    "paths = dict(\n",
    "    a=strainA_path,\n",
    "    b=strainB_path,\n",
    "    c=strainC_path,\n",
    "    x=ancestral_path,\n",
    ")\n",
    "mean_depths = dict(\n",
    "    a=[100, 0, 0, 0, 0, 0, 0],\n",
    "    b=[0, 10, 0, 0, 0, 0, 0],\n",
    "    c=[0, 0, 1, 1, 1, 1, 1],\n",
    "    x=[0, 0, 0, 0, 0, 0, 0],\n",
    ")\n",
    "\n",
    "\n",
    "nnodes = max(itertools.chain(*paths.values())) + 1\n",
    "nsamples = 7\n",
    "\n",
    "expected_depths = np.zeros((nsamples, nnodes))\n",
    "for p in paths:\n",
    "    expected_depths[:, paths[p]] += np.outer(mean_depths[p], np.ones(len(paths[p])))\n",
    "    \n",
    "_depths = sp.stats.poisson(mu=expected_depths * 10).rvs()\n",
    "\n",
    "g10 = new_graph_from_merged_paths(\n",
    "    paths.values(),\n",
    "    depths=_depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "draw_graph(g10)\n",
    "print(g10)\n",
    "mutate_compress_all_unitigs(g10)\n",
    "\n",
    "for i in range(6):\n",
    "    draw_graph(g10)\n",
    "    print(g10)\n",
    "    # print(i, g10)\n",
    "    mutate_add_flows(g10, estimate_all_flows(g10, maxiter=1000, eps=1e-3))\n",
    "    # print(total_depth_length(g10))\n",
    "    mutate_split_all_nodes(g10, partial(splits_from_sparse_encoding3, eps=50))\n",
    "    # print(total_depth_length(g10))\n",
    "    mutate_compress_all_unitigs(g10)\n",
    "    # print(total_depth_length(g10))\n",
    "draw_graph(g10)\n",
    "print(g10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = pd.DataFrame(dict(\n",
    "    in_degree=g10.degree_property_map('in').a,\n",
    "    out_degree=g10.degree_property_map('out').a,\n",
    "    length=g10.vp.length.a,\n",
    "))\n",
    "vp = vp.join(pd.DataFrame(get_depth_matrix(g10).T, columns=['a', 'b', 'c', 'c1', 'c2', 'c3', 'c4']))\n",
    "vp = vp.join(vp[['a', 'b', 'c', 'c1', 'c2', 'c3', 'c4']].multiply(vp.length, axis=0), rsuffix='_x_len')\n",
    "vp.sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_seqs = vp.length.sort_values(ascending=False).head(10).index\n",
    "vp.loc[longest_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = longest_seqs\n",
    "\n",
    "fig, axs = plt.subplots(len(query_list), len(ref_list), figsize=(3 * len(ref_list), 3 * len(query_list)), sharex=True, sharey=True)\n",
    "axs = axs.reshape((len(query_list), len(ref_list)))\n",
    "\n",
    "for (i, query_idx), (j, ref_idx), in product(enumerate(query_list), enumerate(ref_list)):\n",
    "    ax = axs[i, j]\n",
    "    ref = paths[ref_idx]\n",
    "    query = g10.vp.sequence[query_idx]\n",
    "    ref = np.asanyarray(ref)\n",
    "    query = np.asanyarray(query)\n",
    "    length = max(len(ref), len(query)) + 1\n",
    "    query = np.pad(query, (0, length - len(query)), constant_values=-1)\n",
    "    ref = np.pad(ref, (0, length - len(ref)), constant_values=-1)\n",
    "    dotplot = sp.spatial.distance.cdist(query.reshape((-1, 1)), ref.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(dotplot.T), marker='o', s=1)\n",
    "    # ax.imshow(dotplot, cmap='Greys', origin='lower')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.annotate(int(vp.loc[query_idx][ref_idx].round()), xy=(0.1, 0.9), xycoords='axes fraction', va='top')\n",
    "        \n",
    "for ref_idx, bottom_row_ax in zip(ref_list, axs[-1,:]):\n",
    "    bottom_row_ax.set_xlabel(ref_idx)\n",
    "    \n",
    "for query_idx, left_column_ax in zip(query_list, axs[:, 0]):\n",
    "    left_column_ax.set_ylabel(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_clike_seqs = vp.c_x_len.sort_values(ascending=False).head(10).index\n",
    "vp.loc[most_clike_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = most_clike_seqs\n",
    "\n",
    "fig, axs = plt.subplots(len(query_list), len(ref_list), figsize=(3 * len(ref_list), 3 * len(query_list)), sharex=True, sharey=True)\n",
    "axs = axs.reshape((len(query_list), len(ref_list)))\n",
    "\n",
    "for (i, query_idx), (j, ref_idx), in product(enumerate(query_list), enumerate(ref_list)):\n",
    "    ax = axs[i, j]\n",
    "    ref = paths[ref_idx]\n",
    "    query = g10.vp.sequence[query_idx]\n",
    "    ref = np.asanyarray(ref)\n",
    "    query = np.asanyarray(query)\n",
    "    length = max(len(ref), len(query))\n",
    "    query = np.pad(query, (0, length - len(query)), constant_values=-1)\n",
    "    ref = np.pad(ref, (0, length - len(ref)), constant_values=-1)\n",
    "    dotplot = sp.spatial.distance.cdist(query.reshape((-1, 1)), ref.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(dotplot.T), marker='o', s=1)\n",
    "    # ax.imshow(dotplot, cmap='Greys', origin='lower')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.annotate(int(vp.loc[query_idx][ref_idx].round()), xy=(0.1, 0.9), xycoords='axes fraction', va='top')\n",
    "for ref_idx, bottom_row_ax in zip(ref_list, axs[-1,:]):\n",
    "    bottom_row_ax.set_xlabel(ref_idx)\n",
    "    \n",
    "for query_idx, left_column_ax in zip(query_list, axs[:, 0]):\n",
    "    left_column_ax.set_ylabel(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "strain_length = 1000\n",
    "num_mutations = strain_length // 2\n",
    "ancestral_path = np.arange(strain_length)\n",
    "\n",
    "strainA_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainA_path = ancestral_path.copy()\n",
    "strainA_path[strainA_mutations] = np.arange(strain_length + 0 * num_mutations, strain_length + 1 * num_mutations)\n",
    "\n",
    "strainB_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainB_path = ancestral_path.copy()\n",
    "strainB_path[strainB_mutations] = np.arange(strain_length + 1 * num_mutations, strain_length + 2 * num_mutations)\n",
    "\n",
    "strainC_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainC_path = ancestral_path.copy()\n",
    "inversion_start = strain_length // 4\n",
    "inversion_end = 3 * strain_length // 4\n",
    "strainC_path[inversion_start : inversion_end] = strainC_path[inversion_end : inversion_start : -1]\n",
    "strainC_path_without_mutations = strainC_path.copy()\n",
    "strainC_path[strainC_mutations] = np.arange(strain_length + 2 * num_mutations, strain_length + 3 * num_mutations)\n",
    "\n",
    "paths = dict(\n",
    "    a=strainA_path,\n",
    "    b=strainB_path,\n",
    "    c=strainC_path,\n",
    "    x=ancestral_path,\n",
    ")\n",
    "mean_depths = dict(\n",
    "    a=[100, 0, 0, 0, 0, 0, 0],\n",
    "    b=[0, 10, 0, 0, 0, 0, 0],\n",
    "    c=[0, 0, 1, 1, 1, 1, 1],\n",
    "    x=[0, 0, 0, 0, 0, 0, 0],\n",
    ")\n",
    "\n",
    "\n",
    "nnodes = max(itertools.chain(*paths.values())) + 1\n",
    "nsamples = 7\n",
    "\n",
    "expected_depths = np.zeros((nsamples, nnodes))\n",
    "for p in paths:\n",
    "    expected_depths[:, paths[p]] += np.outer(mean_depths[p], np.ones(len(paths[p])))\n",
    "    \n",
    "_depths = sp.stats.poisson(mu=expected_depths * 10).rvs()\n",
    "\n",
    "g11 = new_graph_from_merged_paths(\n",
    "    paths.values(),\n",
    "    depths=_depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "draw_graph(g11)\n",
    "print(g11)\n",
    "mutate_compress_all_unitigs(g11)\n",
    "\n",
    "for i in range(7):\n",
    "    draw_graph(g11)\n",
    "    print(g11)\n",
    "    # print(i, g11)\n",
    "    mutate_add_flows(g11, estimate_all_flows(g11, maxiter=1000, eps=1e-3))\n",
    "    # print(total_depth_length(g11))\n",
    "    mutate_split_all_nodes(g11, partial(splits_from_sparse_encoding3, eps=20))\n",
    "    # print(total_depth_length(g11))\n",
    "    mutate_compress_all_unitigs(g11)\n",
    "    # print(total_depth_length(g11))\n",
    "draw_graph(g11)\n",
    "print(g11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = pd.DataFrame(dict(\n",
    "    in_degree=g11.degree_property_map('in').a,\n",
    "    out_degree=g11.degree_property_map('out').a,\n",
    "    length=g11.vp.length.a,\n",
    "))\n",
    "vp = vp.join(pd.DataFrame(get_depth_matrix(g11).T, columns=['a', 'b', 'c', 'c1', 'c2', 'c3', 'c4']))\n",
    "vp = vp.join(vp[['a', 'b', 'c', 'c1', 'c2', 'c3', 'c4']].multiply(vp.length, axis=0), rsuffix='_x_len')\n",
    "vp.sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_alike_seqs = vp.a_x_len.sort_values(ascending=False).head(10).index\n",
    "vp.loc[most_alike_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_blike_seqs = vp.b_x_len.sort_values(ascending=False).head(10).index\n",
    "vp.loc[most_blike_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_clike_seqs = vp.c_x_len.sort_values(ascending=False).head(10).index\n",
    "vp.loc[most_clike_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_clike_seqs = vp.c_x_len.sort_values(ascending=False).head(10).index\n",
    "vp.loc[most_clike_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = [698, 743, 439, 584, 1218, 845, 1173, 957, 141, 48, 473]\n",
    "\n",
    "fig, axs = plt.subplots(len(query_list), len(ref_list), figsize=(3 * len(ref_list), 3 * len(query_list)), sharex=True, sharey=True)\n",
    "axs = axs.reshape((len(query_list), len(ref_list)))\n",
    "\n",
    "for (i, query_idx), (j, ref_idx), in product(enumerate(query_list), enumerate(ref_list)):\n",
    "    ax = axs[i, j]\n",
    "    ref = paths[ref_idx]\n",
    "    query = g11.vp.sequence[query_idx]\n",
    "    ref = np.asanyarray(ref)\n",
    "    query = np.asanyarray(query)\n",
    "    length = max(len(ref), len(query)) + 1\n",
    "    query = np.pad(query, (0, length - len(query)), constant_values=-1)\n",
    "    ref = np.pad(ref, (0, length - len(ref)), constant_values=-1)\n",
    "    dotplot = sp.spatial.distance.cdist(query.reshape((-1, 1)), ref.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(dotplot.T), marker='o', s=1)\n",
    "    # ax.imshow(dotplot, cmap='Greys', origin='lower')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.annotate(int(vp.loc[query_idx][ref_idx].round()), xy=(0.1, 0.9), xycoords='axes fraction', va='top')\n",
    "for ref_idx, bottom_row_ax in zip(ref_list, axs[-1,:]):\n",
    "    bottom_row_ax.set_xlabel(ref_idx)\n",
    "    \n",
    "for query_idx, left_column_ax in zip(query_list, axs[:, 0]):\n",
    "    left_column_ax.set_ylabel(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = most_clike_seqs\n",
    "\n",
    "fig, axs = plt.subplots(len(query_list), len(ref_list), figsize=(3 * len(ref_list), 3 * len(query_list)), sharex=True, sharey=True)\n",
    "axs = axs.reshape((len(query_list), len(ref_list)))\n",
    "\n",
    "for (i, query_idx), (j, ref_idx), in product(enumerate(query_list), enumerate(ref_list)):\n",
    "    ax = axs[i, j]\n",
    "    ref = paths[ref_idx]\n",
    "    query = g11.vp.sequence[query_idx]\n",
    "    ref = np.asanyarray(ref)\n",
    "    query = np.asanyarray(query)\n",
    "    length = max(len(ref), len(query)) + 1\n",
    "    query = np.pad(query, (0, length - len(query)), constant_values=-1)\n",
    "    ref = np.pad(ref, (0, length - len(ref)), constant_values=-1)\n",
    "    dotplot = sp.spatial.distance.cdist(query.reshape((-1, 1)), ref.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(dotplot.T), marker='o', s=1)\n",
    "    # ax.imshow(dotplot, cmap='Greys', origin='lower')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.annotate(int(vp.loc[query_idx][ref_idx].round()), xy=(0.1, 0.9), xycoords='axes fraction', va='top')\n",
    "for ref_idx, bottom_row_ax in zip(ref_list, axs[-1,:]):\n",
    "    bottom_row_ax.set_xlabel(ref_idx)\n",
    "    \n",
    "for query_idx, left_column_ax in zip(query_list, axs[:, 0]):\n",
    "    left_column_ax.set_ylabel(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    [0, 1, 2, 0],\n",
    "    [0, 3, 4, 0],\n",
    "    [0, 0],\n",
    "]\n",
    "\n",
    "nnodes = max(itertools.chain(*paths)) + 1\n",
    "\n",
    "depths = np.array([\n",
    "    [5, 5, 5, 0, 0],\n",
    "    [4, 0, 0, 4, 4],\n",
    "    [5, 1, 1, 1, 1],\n",
    "    [11, 2, 2, 2, 2],\n",
    "])\n",
    "nsamples = depths.shape[0]\n",
    "\n",
    "g = new_graph_from_merged_paths(\n",
    "    paths,\n",
    "    depths=depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "mutate_add_flows(g, estimate_all_flows(g, eps=1e-4, maxiter=1000))\n",
    "v = 0\n",
    "\n",
    "def splits_from_sparse_encoding4(g, v, eps=1e-2):\n",
    "    # Group Matching Pursuit (GMP)\n",
    "    # Inspired by https://arxiv.org/pdf/1812.10538.pdf\n",
    "    in_neighbors = list(sorted(g.get_in_neighbors(v)))\n",
    "    num_in_neighbors = len(in_neighbors)\n",
    "    in_neighbors_label = {k: v for k, v in enumerate(in_neighbors)}\n",
    "    in_neighbors_onehot = {k: v for k, v in zip(in_neighbors, np.eye(num_in_neighbors))}\n",
    "    in_neighbors_onehot[None] = np.zeros(num_in_neighbors)\n",
    "\n",
    "    out_neighbors = list(sorted(g.get_out_neighbors(v)))\n",
    "    num_out_neighbors = len(out_neighbors)\n",
    "    out_neighbors_label = {k: v for k, v in enumerate(out_neighbors)}\n",
    "    out_neighbors_onehot = {k: v for k, v in zip(out_neighbors, np.eye(num_out_neighbors))}\n",
    "    out_neighbors_onehot[None] = np.zeros(num_out_neighbors)\n",
    "\n",
    "    in_neighbor_flow = []\n",
    "    for u in in_neighbors:\n",
    "        in_neighbor_flow.append(g.ep.flow[g.edge(u, v)])\n",
    "    # in_neighbor_flow = np.stack(in_neighbor_flow)\n",
    "\n",
    "    out_neighbor_flow = []\n",
    "    for w in out_neighbors:\n",
    "        out_neighbor_flow.append(g.ep.flow[g.edge(v, w)])\n",
    "    # out_neighbor_flow = np.stack(out_neighbor_flow)\n",
    "\n",
    "    in_neighbor_code = []\n",
    "    out_neighbor_code = []\n",
    "    split_idx = {}\n",
    "    for i, (u, w) in enumerate(product(in_neighbors + [None], out_neighbors + [None])):\n",
    "        if (u, w) == (None, None):\n",
    "            continue\n",
    "            # NOTE: I DON'T include a node-only atom,\n",
    "            # because I'll return this always and I don't want\n",
    "            # to double-count.\n",
    "        in_neighbor_code.append(in_neighbors_onehot[u])\n",
    "        out_neighbor_code.append(out_neighbors_onehot[w])\n",
    "        split_idx[i] = (u, w)\n",
    "    in_neighbor_code = np.stack(in_neighbor_code)\n",
    "    out_neighbor_code = np.stack(out_neighbor_code)\n",
    "\n",
    "    depth_row = g.vp.depth[v].a\n",
    "    obs = np.stack(in_neighbor_flow + [depth_row] + out_neighbor_flow).T\n",
    "    code = np.concatenate([\n",
    "        in_neighbor_code,\n",
    "        np.ones((in_neighbor_code.shape[0], 1)),\n",
    "        out_neighbor_code\n",
    "    ], axis=1)\n",
    "    # print(obs.round(2))\n",
    "    # print(code)\n",
    "\n",
    "    resid = obs\n",
    "    atoms = []\n",
    "    dictionary = np.zeros_like(code)\n",
    "    encoding = np.zeros((obs.shape[0], dictionary.shape[0]))\n",
    "    for _ in range(code.shape[0]):\n",
    "        loss = np.abs(resid).sum()\n",
    "        # print('-----', loss)\n",
    "        dot = resid @ code.T\n",
    "        # print(dot.round(2))\n",
    "        next_atom = dot.sum(0).argmax()\n",
    "        # TODO: Decide if I want to add the atom with the single element\n",
    "        # or the atom with the largest _summed_ dot-product.\n",
    "        if dot[:, next_atom].sum() <= eps:\n",
    "            # print(\"No atoms to add.\", dot.round(5))\n",
    "            break\n",
    "        atoms.append(next_atom)\n",
    "        # print(atoms)\n",
    "        dictionary[atoms[-1]] = code[atoms[-1]]\n",
    "        # print(dictionary)\n",
    "        # encoding = sparse_encode(obs, dictionary=dictionary, algorithm='lasso_lars', positive=True, alpha=0.)\n",
    "        encoding, _, _ = non_negative_factorization(obs, n_components=dictionary.shape[0], H=dictionary, update_H=False, alpha_W=0)\n",
    "        # print(encoding.round(2))\n",
    "        resid = obs - encoding @ code\n",
    "        # print('-----', split_idx[atoms[-1]], dot_sum.max(), np.abs(resid).sum())\n",
    "        # print(resid.round(2))\n",
    "    # print(depth_row, encoding.sum(1))\n",
    "    active_components = np.arange(encoding.shape[1])[encoding.sum(0) != 0]\n",
    "    for i in atoms:\n",
    "        u, w = split_idx[i]\n",
    "        yield Split(u, v, w), g.vp.length[v], encoding[:, i]\n",
    "    remaining_depth = g.vp.depth[v] - encoding.sum(1)\n",
    "    if not np.allclose(remaining_depth, 0):\n",
    "        yield Split(None, v, None), g.vp.length[v], g.vp.depth[v] - encoding.sum(1)\n",
    "\n",
    "list(splits_from_sparse_encoding4(g, 0, eps=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "strain_length = 1000\n",
    "num_mutations = strain_length // 2\n",
    "ancestral_path = np.arange(strain_length)\n",
    "\n",
    "strainA_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainA_path = ancestral_path.copy()\n",
    "strainA_path[strainA_mutations] = np.arange(strain_length + 0 * num_mutations, strain_length + 1 * num_mutations)\n",
    "\n",
    "strainB_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainB_path = ancestral_path.copy()\n",
    "strainB_path[strainB_mutations] = np.arange(strain_length + 1 * num_mutations, strain_length + 2 * num_mutations)\n",
    "\n",
    "strainC_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainC_path = ancestral_path.copy()\n",
    "inversion_start = strain_length // 4\n",
    "inversion_end = 3 * strain_length // 4\n",
    "strainC_path[inversion_start : inversion_end] = strainC_path[inversion_end : inversion_start : -1]\n",
    "strainC_path_without_mutations = strainC_path.copy()\n",
    "strainC_path[strainC_mutations] = np.arange(strain_length + 2 * num_mutations, strain_length + 3 * num_mutations)\n",
    "\n",
    "paths = dict(\n",
    "    a=strainA_path,\n",
    "    b=strainB_path,\n",
    "    c=strainC_path,\n",
    "    x=ancestral_path,\n",
    ")\n",
    "mean_depths = dict(\n",
    "    a=[100, 0, 0, 0, 0, 0, 0],\n",
    "    b=[0, 10, 0, 0, 0, 0, 0],\n",
    "    c=[0, 0, 1, 1, 1, 1, 1],\n",
    "    x=[0, 0, 0, 0, 0, 0, 0],\n",
    ")\n",
    "\n",
    "\n",
    "nnodes = max(itertools.chain(*paths.values())) + 1\n",
    "nsamples = 7\n",
    "\n",
    "expected_depths = np.zeros((nsamples, nnodes))\n",
    "for p in paths:\n",
    "    expected_depths[:, paths[p]] += np.outer(mean_depths[p], np.ones(len(paths[p])))\n",
    "    \n",
    "_depths = sp.stats.poisson(mu=expected_depths * 10).rvs()\n",
    "\n",
    "g12 = new_graph_from_merged_paths(\n",
    "    paths.values(),\n",
    "    depths=_depths,\n",
    "    lengths=np.array([1] * nnodes),\n",
    ")\n",
    "draw_graph(g12)\n",
    "print(g12)\n",
    "mutate_compress_all_unitigs(g12)\n",
    "\n",
    "for i in range(7):\n",
    "    draw_graph(g12)\n",
    "    print(g12)\n",
    "    # print(i, g12)\n",
    "    mutate_add_flows(g12, estimate_all_flows(g12, maxiter=1000, eps=1e-3))\n",
    "    # print(total_depth_length(g12))\n",
    "    mutate_split_all_nodes(g12, partial(splits_from_sparse_encoding4, eps=20))\n",
    "    # print(total_depth_length(g12))\n",
    "    mutate_compress_all_unitigs(g12)\n",
    "    # print(total_depth_length(g12))\n",
    "draw_graph(g12)\n",
    "print(g12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = pd.DataFrame(dict(\n",
    "    in_degree=g12.degree_property_map('in').a,\n",
    "    out_degree=g12.degree_property_map('out').a,\n",
    "    length=g12.vp.length.a,\n",
    "))\n",
    "vp = vp.join(pd.DataFrame(get_depth_matrix(g11).T, columns=['a', 'b', 'c', 'c1', 'c2', 'c3', 'c4']))\n",
    "vp = vp.join(vp[['a', 'b', 'c', 'c1', 'c2', 'c3', 'c4']].multiply(vp.length, axis=0), rsuffix='_x_len')\n",
    "vp.sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_clike_seqs = vp.c_x_len.sort_values(ascending=False).head(10).index\n",
    "vp.loc[most_clike_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = most_clike_seqs\n",
    "\n",
    "fig, axs = plt.subplots(len(query_list), len(ref_list), figsize=(3 * len(ref_list), 3 * len(query_list)), sharex=True, sharey=True)\n",
    "axs = axs.reshape((len(query_list), len(ref_list)))\n",
    "\n",
    "for (i, query_idx), (j, ref_idx), in product(enumerate(query_list), enumerate(ref_list)):\n",
    "    ax = axs[i, j]\n",
    "    ref = paths[ref_idx]\n",
    "    query = g11.vp.sequence[query_idx]\n",
    "    ref = np.asanyarray(ref)\n",
    "    query = np.asanyarray(query)\n",
    "    length = max(len(ref), len(query)) + 1\n",
    "    query = np.pad(query, (0, length - len(query)), constant_values=-1)\n",
    "    ref = np.pad(ref, (0, length - len(ref)), constant_values=-1)\n",
    "    dotplot = sp.spatial.distance.cdist(query.reshape((-1, 1)), ref.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(dotplot.T), marker='o', s=1)\n",
    "    # ax.imshow(dotplot, cmap='Greys', origin='lower')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.annotate(int(vp.loc[query_idx][ref_idx].round()), xy=(0.1, 0.9), xycoords='axes fraction', va='top')\n",
    "for ref_idx, bottom_row_ax in zip(ref_list, axs[-1,:]):\n",
    "    bottom_row_ax.set_xlabel(ref_idx)\n",
    "    \n",
    "for query_idx, left_column_ax in zip(query_list, axs[:, 0]):\n",
    "    left_column_ax.set_ylabel(query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}