{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "_os.chdir(_os.environ['PROJECT_ROOT'])\n",
    "print(_os.path.realpath(_os.path.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool as gt\n",
    "import graph_tool.draw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import operator\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from sklearn.decomposition import non_negative_factorization\n",
    "from functools import partial\n",
    "import itertools\n",
    "from warnings import warn\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph generation\n",
    "\n",
    "def path_to_edgelist(path):\n",
    "    u = path[0]\n",
    "    edges = []\n",
    "    for v in path[1:]:\n",
    "        edges.append((u, v))\n",
    "        u = v\n",
    "    return edges\n",
    "\n",
    "\n",
    "def is_subseq(x, y):\n",
    "    # Borrowed from https://stackoverflow.com/a/24017747/1951857\n",
    "    return all(any(c == ch for c in y) for ch in x)\n",
    "\n",
    "\n",
    "def single_stranded_graph_from_merged_paths(paths, lengths, depths):\n",
    "    g = gt.Graph()\n",
    "    all_edges = []\n",
    "    for p in paths:\n",
    "        all_edges.extend(path_to_edgelist(p))\n",
    "    g.add_edge_list(set(all_edges))\n",
    "    g.vp['depth'] = g.new_vp('vector<float>')\n",
    "    g.vp.depth.set_2d_array(depths)\n",
    "    g.vp['length'] = g.new_vp('int', lengths)  \n",
    "    g.gp['nsample'] = g.new_gp('int', len(depths))\n",
    "    g.vp['sequence'] = g.new_vp('object', vals=[[k] for k in range(g.num_vertices())])\n",
    "    g.ep['flow'] = g.new_ep('vector<float>', val=[1] * g.gp.nsample)\n",
    "    return g\n",
    "\n",
    "\n",
    "def single_stranded_graph_with_simulated_depth(paths, depths, length=None, scale_depth_by=1):\n",
    "    if length is None:\n",
    "        length = defaultdict(lambda: 1)\n",
    "\n",
    "    nvertices = max(itertools.chain(*paths.values())) + 1\n",
    "    nsamples = max(len(x) for x in depths.values())\n",
    "    \n",
    "    vertex_length = np.array([length[i] for i in range(nvertices)])\n",
    "\n",
    "    expected_depths = np.zeros((nsamples, nvertices))\n",
    "    for p in paths:\n",
    "        expected_depths[:, paths[p]] += np.outer(np.array(depths[p]) * scale_depth_by, np.ones(len(paths[p])))\n",
    "    \n",
    "    # TODO: Consider using nbinom\n",
    "    # # See docs for sp.stats.nbinom\n",
    "    # sigma_sq = expected_depths + dispersion * expected_depths**2\n",
    "    # p = expected_depths / sigma_sq\n",
    "    # n = expected_depths**2 / (sigma_sq - expected_depths)\n",
    "    _depths = sp.stats.poisson(mu=expected_depths).rvs()\n",
    "\n",
    "    g = single_stranded_graph_from_merged_paths(\n",
    "        paths.values(),\n",
    "        depths=_depths,\n",
    "        lengths=vertex_length,\n",
    "    )\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph statistics\n",
    "\n",
    "def depth_matrix(g, vs=None, samples=None):\n",
    "    if vs is None:\n",
    "        vs = g.get_vertices()\n",
    "    if samples is None:\n",
    "        samples = np.arange(g.gp.nsample)\n",
    "    depth = g.vp.depth.get_2d_array(samples)\n",
    "    return depth[:, vs]\n",
    "\n",
    "\n",
    "def total_length_x_depth(g):\n",
    "    return (depth_matrix(g) * g.vp.length.a).sum()\n",
    "\n",
    "\n",
    "def edit_ratio(ref, query):\n",
    "    diff = difflib.SequenceMatcher(a=ref, b=query)\n",
    "    return diff.ratio() * (len(diff.a) + len(diff.b)) / (2 * len(diff.b))\n",
    "\n",
    "\n",
    "def vertex_description(g, refs=None):\n",
    "    if refs is None:\n",
    "        refs = []\n",
    "    vertex = pd.DataFrame(dict(\n",
    "        in_degree=g.degree_property_map('in').a,\n",
    "        out_degree=g.degree_property_map('out').a,\n",
    "        length=g.vp.length.a,\n",
    "    ))\n",
    "    depth = pd.DataFrame(depth_matrix(g).T)\n",
    "    depth.rename(columns=lambda i: f\"d{i}\")\n",
    "    return vertex.join(depth)\n",
    "\n",
    "def scale_ep(ep, maximum=2):\n",
    "    g = ep.get_graph()\n",
    "    return g.new_edge_property('float', vals=ep.a * maximum / ep.a.max())\n",
    "\n",
    "def scale_vp(vp, maximum=10):\n",
    "    g = vp.get_graph()\n",
    "    return g.new_vertex_property('float', vals=vp.a * maximum / vp.a.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "def draw_graph(g, output_size=(300, 300), ink_scale=0.8, **kwargs):\n",
    "    kwargs = dict(\n",
    "        vertex_fill_color=g.new_vertex_property('float', vals=np.linspace(0, 1, num=max(g.get_vertices()) + 1)),\n",
    "        vertex_text=g.vertex_index,\n",
    "    ) | kwargs\n",
    "    return gt.draw.graph_draw(g, output_size=output_size, ink_scale=ink_scale, **kwargs)\n",
    "\n",
    "\n",
    "def dotplot(pathA, pathB, ax=None, **scatter_kws):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    pathA = np.asanyarray(pathA)\n",
    "    pathB = np.asanyarray(pathB)\n",
    "    length = max(len(pathA), len(pathB)) + 1\n",
    "    pathA = np.pad(pathA, (0, length - len(pathA)), constant_values=-1)\n",
    "    pathA = np.pad(pathA, (0, length - len(pathA)), constant_values=-1)\n",
    "    match = sp.spatial.distance.cdist(pathA.reshape((-1, 1)), pathB.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(match.T), **(dict(marker='o', s=1) | scatter_kws))\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitigs\n",
    "\n",
    "def edge_has_no_siblings(g):\n",
    "    \"Check whether upstream or downstream sibling edges exist for every edge.\"\n",
    "    vs = g.get_vertices()\n",
    "    v_in_degree = g.new_vertex_property('int', vals=g.get_in_degrees(vs))\n",
    "    v_out_degree = g.new_vertex_property('int', vals=g.get_out_degrees(vs))\n",
    "    e_num_in_siblings = gt.edge_endpoint_property(g, v_in_degree, 'target')\n",
    "    e_num_out_siblings = gt.edge_endpoint_property(g, v_out_degree, 'source')\n",
    "    e_has_no_sibling_edges = g.new_edge_property('bool', (e_num_in_siblings.a <= 1) & (e_num_out_siblings.a <= 1))\n",
    "    return e_has_no_sibling_edges\n",
    "\n",
    "\n",
    "def vertex_does_not_have_both_multiple_in_and_multiple_out(g):\n",
    "    vs = g.get_vertices()\n",
    "    return g.new_vertex_property('bool', vals=(\n",
    "        (g.get_in_degrees(vs) <= 1)\n",
    "        | (g.get_out_degrees(vs) <= 1)\n",
    "    ))\n",
    "\n",
    "\n",
    "def label_maximal_unitigs_old(g):\n",
    "    \"Assign unitig indices to vertices in maximal unitigs.\"\n",
    "    no_sibling_edges = edge_has_no_siblings(g)\n",
    "    # Since any vertex that has both multiple in _and_ multiple out\n",
    "    # edges cannot be part of a larger maximal unitig,\n",
    "    # we could filter out these vertices, at the same time as we\n",
    "    # are filtering out the edges with siblings.\n",
    "    # Potentially this would make the component labeling step\n",
    "    # much faster.\n",
    "    both_sides_branch = vertex_does_not_have_both_multiple_in_and_multiple_out(g)\n",
    "    # TODO: Double check, if this has any implications for the\n",
    "    # \"unitig-ness\" of its neighbors. I _think_\n",
    "    # if we mark edges with siblings before filtering out\n",
    "    # these nodes we should be good.\n",
    "    g_filt = gt.GraphView(\n",
    "        g,\n",
    "        efilt=no_sibling_edges,\n",
    "        vfilt=both_sides_branch,\n",
    "        directed=True\n",
    "    )\n",
    "    # g_filt_undirected = gt.GraphView(g_filt, directed=False)\n",
    "    # Since we've filtered out the both_sides_branch vertices,\n",
    "    # the labels PropertyMap would include a bunch of the default value (0)\n",
    "    # for these. Instead, we set everything not labeled to -1, now a magic\n",
    "    # value for nodes definitely not in maximal unitigs.\n",
    "    labels = g.new_vertex_property('int', val=-1)\n",
    "    labels, counts = gt.topology.label_components(g_filt, directed=False, vprop=labels)\n",
    "    return labels, counts, g_filt\n",
    "\n",
    "\n",
    "def label_maximal_unitigs(g):\n",
    "    \"Assign unitig indices to vertices in maximal unitigs.\"\n",
    "    no_sibling_edges = edge_has_no_siblings(g)\n",
    "    # # TODO: Since any vertex that has both multiple in _and_ multiple out\n",
    "    # # edges cannot be part of a larger maximal unitig,\n",
    "    # # we could filter out these vertices, at the same time as we\n",
    "    # # are filtering out the edges with siblings.\n",
    "    # # Potentially this would make the component labeling step\n",
    "    # # much faster.\n",
    "    # both_sides_branch = vertex_does_not_have_both_multiple_in_and_multiple_out(g)\n",
    "    g_filt = gt.GraphView(\n",
    "        g,\n",
    "        efilt=no_sibling_edges,\n",
    "        # vfilt=both_sides_branch,\n",
    "        directed=True\n",
    "    )\n",
    "    # g_filt_undirected = gt.GraphView(g_filt, directed=False)\n",
    "    # Since we've filtered out the both_sides_branch vertices,\n",
    "    # the labels PropertyMap would include a bunch of the default value (0)\n",
    "    # for these. Instead, we set everything not labeled to -1, now a magic\n",
    "    # value for nodes definitely not in maximal unitigs.\n",
    "    labels = g.new_vertex_property('int', val=-1)\n",
    "    labels, counts = gt.topology.label_components(g_filt, directed=False, vprop=labels)\n",
    "    return labels, counts, g_filt\n",
    "\n",
    "\n",
    "# def is_simple_cycle_unitig(g, vfilt):\n",
    "#     # TODO: Make this function WAY more efficient.\n",
    "#     g_filt = gt.GraphView(g, vfilt=vfilt)\n",
    "#     vs = list(g_filt.iter_vertices())\n",
    "#     path = list(gt.topology.all_paths(g_filt, vs[0], vs[0]))\n",
    "#     if not len(path) == 1:\n",
    "#         return False\n",
    "#     elif set(path[0]) != set(vs):\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "\n",
    "    \n",
    "def maximal_unitigs(g, include_singletons=False):\n",
    "    labels, counts, g_filt = label_maximal_unitigs(g)\n",
    "    for i, c in enumerate(counts):\n",
    "        if (c == 1) and (not include_singletons):\n",
    "            continue\n",
    "        subgraph = gt.GraphView(g_filt, vfilt=(labels.a == i))\n",
    "        if not gt.topology.is_DAG(subgraph):\n",
    "            # It's a cycle\n",
    "            # (and an independent component, at that, since\n",
    "            # the subgraph would no longer be a cycle, otherwise.\n",
    "            # Pick an arbitrary vertex as the start\n",
    "            v = next(subgraph.iter_vertices())\n",
    "            # Trace a path from v -> v.\n",
    "            # Should be only one path, so we'll only generate one iteration.\n",
    "            # (TODO: Consider asserting this),\n",
    "            paths = list(gt.topology.all_paths(subgraph, v, v))\n",
    "            assert len(paths) == 1\n",
    "            vs = list(paths[0])\n",
    "            yield vs[:-1], True\n",
    "        else:\n",
    "            vs = list(gt.topology.topological_sort(subgraph))\n",
    "            # If there's an edge from the last vertex to the\n",
    "            # first in the original graph, then it's a cycle.\n",
    "            # TODO: Confirm I actually want to report this as\n",
    "            # a cycle. I may have meant that it's an unbroken cycle...\n",
    "            if g.edge(vs[-1], vs[0]):\n",
    "                yield vs, True\n",
    "            else:\n",
    "                yield vs, False\n",
    "\n",
    "\n",
    "def list_unitig_neighbors(g, vs):\n",
    "    \"The in and out neighbors of a unitig path.\"\n",
    "    all_ins = set(g.get_in_neighbors(vs[0]))\n",
    "    all_outs = set(g.get_out_neighbors(vs[-1]))\n",
    "    return list(set(all_ins) - set(vs)), list(set(all_outs) - set(vs))\n",
    "\n",
    "\n",
    "def mutate_add_compressed_unitig_vertex(g, vs, is_cycle, drop_vs=False):\n",
    "    v = int(g.add_vertex())\n",
    "    nsample = g.gp.nsample\n",
    "    in_neighbors, out_neighbors = list_unitig_neighbors(g, vs)\n",
    "    g.add_edge_list((neighbor, v) for neighbor in in_neighbors)\n",
    "    g.add_edge_list((v, neighbor) for neighbor in out_neighbors)\n",
    "    g.vp.length.a[v] = g.vp.length.a[vs].sum()\n",
    "    g.vp.sequence[v] = reduce(operator.add, (g.vp.sequence[u] for u in vs), [])\n",
    "    g.vp.depth[v] = (\n",
    "        (\n",
    "            depth_matrix(g, vs)\n",
    "            * g.vp.length.a[vs]\n",
    "        ).sum(1) / g.vp.length.a[v]\n",
    "    )\n",
    "    assert np.allclose(\n",
    "        (depth_matrix(g, vs) * g.vp.length.a[vs]).sum(1),\n",
    "        (depth_matrix(g, [v]) * g.vp.length.a[v]).sum(1)\n",
    "    )\n",
    "    if is_cycle:\n",
    "        g.add_edge(v, v)\n",
    "    for old_v in vs:\n",
    "        g.clear_vertex(old_v)\n",
    "    return g\n",
    "\n",
    "\n",
    "def mutate_compress_all_unitigs(g):\n",
    "    unitig_list = maximal_unitigs(g, include_singletons=False)\n",
    "    all_vs = []\n",
    "    for i, (vs, is_cycle) in enumerate(unitig_list):\n",
    "        assert len(vs) > 1  # All len(vs) == 1 should be removed by include_singletons=False.\n",
    "        mutate_add_compressed_unitig_vertex(g, vs, is_cycle)\n",
    "        all_vs.extend(vs)\n",
    "    g.remove_vertex(set(all_vs), fast=True)\n",
    "    # I think, but am not sure, that the number of nodes removed will always equal the number of edges removed.\n",
    "    return g\n",
    "\n",
    "def label_self_looping_vertices(g):\n",
    "    return (\n",
    "        gt.incident_edges_op(\n",
    "            g,\n",
    "            direction='in',\n",
    "            op='max',\n",
    "            eprop=gt.topology.label_self_loops(g, mark_only=True),\n",
    "        )\n",
    "    )\n",
    "\n",
    "def describe_nodes(g):\n",
    "    description = pd.DataFrame(dict(\n",
    "        length=g.vp.length.a,\n",
    "        in_degree=g.degree_property_map('in').a,\n",
    "        out_degree=g.degree_property_map('out').a,\n",
    "        circular=label_self_looping_vertices(g).a,\n",
    "        sequence=[g.vp.sequence[v] for v in g.iter_vertices()],\n",
    "    ))\n",
    "    depth = pd.DataFrame(depth_matrix(g).T)\n",
    "    return description.join(depth)\n",
    "\n",
    "\n",
    "def mutate_extract_singletons(g):\n",
    "    nodes = describe_nodes(g)\n",
    "    singletons = idxwhere((nodes.in_degree + nodes.out_degree - 2 * nodes.circular) == 0)\n",
    "    g.remove_vertex(singletons, fast=True)\n",
    "    return g, nodes.loc[singletons].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flows\n",
    "\n",
    "def estimate_flow(f0, d, weight=None, eps=1e-2, maxiter=100):\n",
    "    if weight is None:\n",
    "        weight = np.ones_like(d)\n",
    "\n",
    "    loss_hist = [np.finfo('float').max]\n",
    "    f = f0\n",
    "    for step_i in range(maxiter):\n",
    "        f_out = f\n",
    "        f_total_out = f_out.sum(1)\n",
    "        d_error_out = f_total_out - d\n",
    "        \n",
    "        f_in = f_out.T\n",
    "        f_total_in = f_in.sum(1)\n",
    "        d_error_in = f_total_in - d\n",
    "        \n",
    "        loss_hist.append(np.square(d_error_out).sum() + np.square(d_error_in).sum())\n",
    "        loss_ratio = (loss_hist[-2] - loss_hist[-1]) / loss_hist[-2]\n",
    "        if loss_ratio < eps:\n",
    "            break\n",
    "            \n",
    "        # NOTE: Because of errstate, this function is NOT threadsafe.\n",
    "        # TODO: Determine if it's safe across multiple processes.\n",
    "        with np.errstate(divide='ignore'):\n",
    "            allocation_out = f_out.T * np.nan_to_num(1 / f_total_out, posinf=1, nan=0)\n",
    "        allocated_d_error_out = (allocation_out * d_error_out).T\n",
    "        with np.errstate(divide='ignore'):\n",
    "            allocation_in = f_in.T * np.nan_to_num(1 / f_total_in, posinf=1, nan=0)\n",
    "        allocated_d_error_in = (allocation_in * d_error_in).T\n",
    "\n",
    "        # The final step is calculated as a average of the in and out error, weighted\n",
    "        # by the node weight.\n",
    "        inv_weight = 1 / weight\n",
    "        mean_allocated_d_error = (\n",
    "            ((allocated_d_error_in * inv_weight).T + (allocated_d_error_out * inv_weight))\n",
    "            * (1 / (inv_weight.reshape((-1, 1)) + (inv_weight.reshape((1, -1)))))\n",
    "        )\n",
    "        \n",
    "        f = (f_out - mean_allocated_d_error)\n",
    "    else:\n",
    "        warn(f\"loss_ratio < eps ({eps}) not achieved in maxiter ({maxiter}) steps. Final loss_ratio={loss_ratio}. Final loss={loss_hist[-1]}.\")\n",
    "    return f\n",
    "\n",
    "\n",
    "def estimate_all_flows(g, eps=1e-3, maxiter=1000, use_weights=True):\n",
    "    flows = []\n",
    "    if use_weights:\n",
    "        weight = g.vp.length.a\n",
    "    else:\n",
    "        weight = None\n",
    "    f0 = sp.sparse.csr_array(gt.spectral.adjacency(g))\n",
    "    dd = depth_matrix(g)\n",
    "    for sample_idx in range(g.gp.nsample):\n",
    "        d = dd[sample_idx]\n",
    "        f = estimate_flow(f0, d, weight=weight, eps=eps, maxiter=maxiter)\n",
    "        flows.append(f)\n",
    "    return flows\n",
    "\n",
    "\n",
    "def mutate_add_flows(g, flows):\n",
    "    props = []\n",
    "    for sample_idx, f in enumerate(flows):\n",
    "        p = g.new_edge_property('float', val=0)\n",
    "        for i, j in g.get_edges():\n",
    "            p[g.edge(i, j)] = f[j, i]\n",
    "        props.append(p)\n",
    "    props = gt.group_vector_property(props)\n",
    "    g.ep['flow'] = props\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node splitting\n",
    "\n",
    "Split = namedtuple('Split', ['u', 'v', 'w'])\n",
    "\n",
    "\n",
    "def splits_from_sparse_encoding(g, v, threshold=1.):\n",
    "    # Compile tables\n",
    "    in_neighbors = list(sorted(g.get_in_neighbors(v)))\n",
    "    num_in_neighbors = len(in_neighbors)\n",
    "    in_neighbors_label = {k: v for k, v in enumerate(in_neighbors)}\n",
    "    in_neighbors_onehot = {k: v for k, v in zip(in_neighbors, np.eye(num_in_neighbors))}\n",
    "    in_neighbors_onehot[None] = np.zeros(num_in_neighbors)\n",
    "\n",
    "    out_neighbors = list(sorted(g.get_out_neighbors(v)))\n",
    "    num_out_neighbors = len(out_neighbors)\n",
    "    out_neighbors_label = {k: v for k, v in enumerate(out_neighbors)}\n",
    "    out_neighbors_onehot = {k: v for k, v in zip(out_neighbors, np.eye(num_out_neighbors))}\n",
    "    out_neighbors_onehot[None] = np.zeros(num_out_neighbors)\n",
    "\n",
    "    in_neighbor_flow = []\n",
    "    for u in in_neighbors:\n",
    "        in_neighbor_flow.append(g.ep.flow[g.edge(u, v)])\n",
    "\n",
    "    out_neighbor_flow = []\n",
    "    for w in out_neighbors:\n",
    "        out_neighbor_flow.append(g.ep.flow[g.edge(v, w)])\n",
    "\n",
    "    in_neighbor_code = []\n",
    "    out_neighbor_code = []\n",
    "    split_idx = {}\n",
    "    for i, (u, w) in enumerate(product(in_neighbors + [None], out_neighbors + [None])):\n",
    "        if (u, w) == (None, None):\n",
    "            # continue\n",
    "            # # NOTE: I DON'T include a node-only atom,\n",
    "            # # because I'll return this always and I don't want\n",
    "            # # to double-count.\n",
    "            # FIXME: Trying out a system where I compare the dotproduct for\n",
    "            # the naked node to the other dot products.\n",
    "            naked_vertex_idx = i\n",
    "            pass\n",
    "        in_neighbor_code.append(in_neighbors_onehot[u])\n",
    "        out_neighbor_code.append(out_neighbors_onehot[w])\n",
    "        split_idx[i] = (u, w)\n",
    "    in_neighbor_code = np.stack(in_neighbor_code)\n",
    "    out_neighbor_code = np.stack(out_neighbor_code)\n",
    "\n",
    "    depth_row = g.vp.depth[v].a\n",
    "    obs = np.stack(in_neighbor_flow + [depth_row] + out_neighbor_flow).T\n",
    "    unnormalized_code = np.concatenate([\n",
    "        in_neighbor_code,\n",
    "        np.ones((in_neighbor_code.shape[0], 1)),\n",
    "        out_neighbor_code\n",
    "    ], axis=1)\n",
    "    code_magnitude = np.sqrt(np.square(unnormalized_code).sum(1, keepdims=True))\n",
    "    code = unnormalized_code / code_magnitude\n",
    "\n",
    "\n",
    "    # Group Matching Pursuit (GMP)\n",
    "    # Inspired by https://arxiv.org/pdf/1812.10538.pdf\n",
    "    resid = obs\n",
    "    atoms = []\n",
    "    dictionary = np.zeros_like(code)\n",
    "    normalized_encoding = np.zeros((obs.shape[0], dictionary.shape[0]))\n",
    "    for _ in range(code.shape[0]):\n",
    "        loss = np.abs(resid).sum()\n",
    "        dot = resid @ code.T\n",
    "        # TODO: Decide how to decide atoms\n",
    "        # next_atom = np.square(dot).sum(0).argmax()\n",
    "        next_atom = dot.argmax() % code.shape[0]\n",
    "        naked_vertex_dot = dot[:, naked_vertex_idx]\n",
    "        if dot[:, next_atom].sum() <= threshold:\n",
    "            break\n",
    "        if next_atom in atoms:\n",
    "            break\n",
    "        if next_atom == naked_vertex_idx:\n",
    "            break  # TODO: Decide if this is a good stopping criterion.\n",
    "        atoms.append(next_atom)\n",
    "        dictionary[atoms[-1]] = code[atoms[-1]]\n",
    "        normalized_encoding, _, _ = non_negative_factorization(obs, n_components=dictionary.shape[0], H=dictionary, update_H=False, alpha_W=0)\n",
    "        resid = obs - normalized_encoding @ code\n",
    "        \n",
    "\n",
    "    # Iterate through atoms as splits.\n",
    "    encoding = normalized_encoding / code_magnitude.T\n",
    "    # import pdb; pdb.set_trace()\n",
    "    for i in atoms:\n",
    "        if i == naked_vertex_idx:\n",
    "            # Don't return the naked vertex here. It'll be returned\n",
    "            # later.\n",
    "            continue\n",
    "        u, w = split_idx[i]\n",
    "        yield Split(u, v, w), g.vp.length[v], encoding[:, i]\n",
    "    # Remaining depth must also include any depth assigned to the naked encoding.\n",
    "    remaining_depth = g.vp.depth[v] - encoding.sum(1) + encoding[:, naked_vertex_idx]\n",
    "    if not np.allclose(remaining_depth, 0):\n",
    "        yield Split(None, v, None), g.vp.length[v], remaining_depth\n",
    "        \n",
    "        \n",
    "def build_tables_from_splits(split_list, start_idx):\n",
    "    \"\"\"Generate edges to and from new, split vertices.\n",
    "    \n",
    "    Note that if splits from adjacent parents are not\n",
    "    reciprocated, no new edge is produced.\n",
    "    \n",
    "    \"\"\"\n",
    "    split_idx = {}\n",
    "    upstream = defaultdict(list)\n",
    "    downstream = defaultdict(list)\n",
    "    length = []\n",
    "    depth = []\n",
    "    for idx, (split, l, d) in enumerate(split_list, start=start_idx):\n",
    "        u, v, w = split\n",
    "        split_idx[split] = idx\n",
    "        upstream[(v, w)].append(split)\n",
    "        downstream[(u, v)].append(split)\n",
    "        depth.append(d)\n",
    "        length.append(l)\n",
    "    assert len(split_list) == len(split_idx)\n",
    "    return split_idx, upstream, downstream, np.array(length), np.array(depth)\n",
    "        \n",
    "        \n",
    "def new_edges_from_splits(split_list, split_idx, upstream, downstream, start_idx):\n",
    "    for v, (split, _, _) in enumerate(split_list, start=start_idx):\n",
    "        u_old, v_old, w_old = split\n",
    "        v = split_idx[split]\n",
    "        \n",
    "        # Upstream edges\n",
    "        if u_old is not None:\n",
    "            yield (u_old, v)\n",
    "        for upstream_split in upstream[(u_old, v_old)]:\n",
    "            u = split_idx[upstream_split]\n",
    "            if u is not None:\n",
    "                yield (u, v)\n",
    "            \n",
    "        # Downstream edges\n",
    "        if w_old is not None:\n",
    "            yield (v, w_old)\n",
    "        for downstream_split in downstream[(v_old, w_old)]:\n",
    "            w = split_idx[downstream_split]\n",
    "            if w is not None:\n",
    "                yield (v, w)\n",
    "            \n",
    "            \n",
    "def mutate_apply_splits(g, split_list):\n",
    "    \"\"\"Add edges and drop any parent vertices that were split.\n",
    "    \n",
    "    \"\"\"\n",
    "    start_idx = g.num_vertices(ignore_filter=True)\n",
    "    split_idx, upstream, downstream, lengths, depths = (\n",
    "        build_tables_from_splits(split_list, start_idx=start_idx)\n",
    "    )\n",
    "    edges_to_add = list(set(new_edges_from_splits(\n",
    "        split_list, split_idx, upstream, downstream, start_idx\n",
    "    )))\n",
    "    \n",
    "    # FIXME:\n",
    "    g_old = g.copy()\n",
    "    \n",
    "    # NOTE: Without adding vertices before edges I can get an IndexError\n",
    "    # running `g.vp.length.a[np.arange(len(lengths)) + start_idx]`.\n",
    "    # I believe this is because one or more split nodes\n",
    "    # are without any new edges, and therefore these nodes don't\n",
    "    # get added implicitly by `g.add_edge_list`.\n",
    "    # When these accidentally hidden nodes are the highest valued\n",
    "    # ones, they also don't get implicitly added due to their index.\n",
    "    # The result is that I'm missing nodes that should actually exist.\n",
    "    # NOTE: This line returns an unassigned generator. I _think_ all the\n",
    "    # nodes are still added, but it's not entirely clear.\n",
    "    # UPDATE: I'm sure the nodes are still added because of the following\n",
    "    # assert.\n",
    "    g.add_vertex(n=max(split_idx.values()) - max(g.get_vertices()))\n",
    "    g.add_edge_list(set(edges_to_add))\n",
    "    assert max(g.get_vertices()) == max(split_idx.values())\n",
    "    \n",
    "    g.vp.length.a[np.arange(len(lengths)) + start_idx] = lengths\n",
    "    new_depth = depth_matrix(g)\n",
    "    new_depth[:, np.arange(len(depths)) + start_idx] = depths.T\n",
    "    g.vp.depth.set_2d_array(new_depth)\n",
    "    for split, _, _ in split_list:\n",
    "        g.vp.sequence[split_idx[split]] = g.vp.sequence[split.v]\n",
    "    # for v in g.iter_vertices():\n",
    "    #     assert g.vp.sequence[v]\n",
    "    vertices_to_drop = set(split.v for (split, _, _) in split_list)\n",
    "    g.remove_vertex(vertices_to_drop, fast=True)\n",
    "    return g\n",
    "\n",
    "\n",
    "def splits_for_all_vertices(g, split_func):\n",
    "    for v in g.vertices():\n",
    "        if (v.in_degree() < 2) and (v.out_degree() < 2):\n",
    "            continue\n",
    "        else:\n",
    "            yield from split_func(g, v)\n",
    "\n",
    "\n",
    "def mutate_split_all_nodes(g, split_func):\n",
    "    # TODO: Vertices with <= 1 local path will be split into just\n",
    "    # themselves pointing trivially at their neighbors.\n",
    "    # These can be dropped as they are effectively a no-op.\n",
    "    split_list = list(splits_for_all_vertices(g, split_func))\n",
    "    if len(split_list) > 0:\n",
    "        g = mutate_apply_splits(g, split_list=split_list)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "gt.seed_rng(2)\n",
    "\n",
    "paths = [\n",
    "    [0, 2, 3],\n",
    "    [1, 2, 4],\n",
    "    # [1, 3, 4, 5],\n",
    "    # [0, 0],\n",
    "]\n",
    "\n",
    "nnodes = max(itertools.chain(*paths)) + 1\n",
    "\n",
    "depths = np.array([\n",
    "    [4, 4, 4, 4, 4],\n",
    "])\n",
    "nsamples = depths.shape[0]\n",
    "\n",
    "_g = single_stranded_graph_from_merged_paths(\n",
    "    paths,\n",
    "    depths=depths,\n",
    "    lengths=[1, 1, 3, 1, 1],\n",
    "    # lengths=np.array([1] * nnodes),\n",
    ")\n",
    "\n",
    "figsize=150\n",
    "pos = draw_graph(_g, output_size=(figsize, figsize))\n",
    "draw_graph(_g, pos=pos, vertex_text=_g.vp.length, output_size=(figsize, figsize))\n",
    "\n",
    "f0 = sp.sparse.csr_array(gt.spectral.adjacency(_g))#.toarray()\n",
    "d = _g.vp.depth.get_2d_array([0])[0]\n",
    "# estimate_flow3(f0, d, eps=1e-2, maxiter=1000, weight=_g.vp.length.a).round(3)\n",
    "# draw_graph(\n",
    "#     _g,\n",
    "#     pos=pos,\n",
    "#     vertex_text=gt.ungroup_vector_property(_g.vp.depth, pos=[0])[0],\n",
    "#     edge_pen_width=gt.ungroup_vector_property(_g.ep.flow, pos=[0])[0],\n",
    "# )\n",
    "\n",
    "mutate_add_flows(_g, estimate_all_flows(_g))\n",
    "draw_graph(\n",
    "    _g,\n",
    "    pos=pos,\n",
    "    vertex_text=gt.ungroup_vector_property(_g.vp.depth, pos=[0])[0],\n",
    "    edge_pen_width=gt.ungroup_vector_property(_g.ep.flow, pos=[0])[0],\n",
    "    output_size=(figsize, figsize),\n",
    ")\n",
    "# mutate_add_flows(_g, estimate_all_flows3(_g))\n",
    "# draw_graph(\n",
    "#     _g,\n",
    "#     pos=pos,\n",
    "#     vertex_text=gt.ungroup_vector_property(_g.vp.depth, pos=[0])[0],\n",
    "#     edge_pen_width=gt.ungroup_vector_property(_g.ep.flow, pos=[0])[0],\n",
    "#     output_size=(figsize, figsize),\n",
    "# )\n",
    "estimate_all_flows(_g)[0].toarray().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "gt.seed_rng(1)\n",
    "\n",
    "paths = dict(\n",
    "    a=[0, 1, 2, 3, 0],\n",
    "    b=[4, 5, 6, 7, 4],\n",
    "    c=[0, 4, 8, 9],\n",
    "    d=[10, 11, 12, 10],\n",
    "    e=[4, 4],\n",
    ")\n",
    "\n",
    "mean_depths = dict(\n",
    "    a=[1],\n",
    "    b=[1],\n",
    "    c=[1],\n",
    "    d=[1],\n",
    "    e=[1]\n",
    ")\n",
    "\n",
    "figsize = 200\n",
    "\n",
    "_g = single_stranded_graph_with_simulated_depth(\n",
    "        paths,\n",
    "        depths=mean_depths, scale_depth_by=20,\n",
    ")\n",
    "\n",
    "labels, counts, _g_filt = label_maximal_unitigs(_g)\n",
    "\n",
    "pos = draw_graph(_g, vertex_fill_color=labels, output_size=(figsize, figsize))\n",
    "draw_graph(_g_filt, pos=pos, vertex_fill_color=labels, vertex_text=labels, output_size=(figsize, figsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "gt.seed_rng(1)\n",
    "\n",
    "strain_length = 10000\n",
    "num_mutations = strain_length // 2\n",
    "ancestral_path = np.arange(strain_length)\n",
    "\n",
    "strainA_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainA_path = ancestral_path.copy()\n",
    "strainA_path[strainA_mutations] = np.arange(strain_length + 0 * num_mutations, strain_length + 1 * num_mutations)\n",
    "\n",
    "strainB_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainB_path = ancestral_path.copy()\n",
    "strainB_path[strainB_mutations] = np.arange(strain_length + 1 * num_mutations, strain_length + 2 * num_mutations)\n",
    "\n",
    "strainC_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainC_path = ancestral_path.copy()\n",
    "strainC_path[strainC_mutations] = np.arange(strain_length + 2 * num_mutations, strain_length + 3 * num_mutations)\n",
    "\n",
    "num_errors = strain_length // 10\n",
    "error_path = np.arange(strain_length + 3 * num_mutations, strain_length + 3 * num_mutations + num_errors, step=0.5)\n",
    "error_path[1::2] = np.random.choice(np.arange(strain_length + 3 * num_mutations), size=num_errors)\n",
    "error_path = list(error_path.astype(int))\n",
    "\n",
    "paths = dict(\n",
    "    x=ancestral_path,\n",
    "    a=strainA_path,\n",
    "    b=strainB_path,\n",
    "    c=strainC_path,\n",
    "    error=error_path,\n",
    "    # self_looping=[10, 10],\n",
    ")\n",
    "\n",
    "sample_replicates = 1\n",
    "mean_depths = dict(\n",
    "    a=[0.90, 0.05, 0.05] * sample_replicates,\n",
    "    b=[0.05, 0.90, 0.05] * sample_replicates,\n",
    "    c=[0.05, 0.05, 0.90] * sample_replicates,\n",
    "    x=[0.00, 0.00, 0.00] * sample_replicates,\n",
    "    error=[0.01, 0.01, 0.01] * sample_replicates,\n",
    "    # self_looping=[1, 1, 1] * sample_replicates,\n",
    ")\n",
    "\n",
    "g = []\n",
    "g.append(\n",
    "    single_stranded_graph_with_simulated_depth(\n",
    "        paths,\n",
    "        depths=mean_depths, scale_depth_by=4,\n",
    "))\n",
    "\n",
    "figsize = 250\n",
    "# draw_graph(g0, output_size=(figsize, figsize))\n",
    "# g1 = mutate_compress_all_unitigs(g0.copy())\n",
    "# mutate_add_flows(g[-1], estimate_all_flows(g[-1]))\n",
    "\n",
    "# pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "# for i in range(3):\n",
    "#     draw_graph(\n",
    "#         g[-1],\n",
    "#         pos=pos,\n",
    "#         vertex_text='',\n",
    "#         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "#         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "#         output_size=(figsize, figsize),\n",
    "#     )\n",
    "    \n",
    "g.append(g[-1].copy())\n",
    "mutate_compress_all_unitigs(g[-1])\n",
    "# mutate_add_flows(g[-1], estimate_all_flows(g[-1]))\n",
    "# pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "# for i in range(3):\n",
    "#     draw_graph(\n",
    "#         g[-1],\n",
    "#         pos=pos,\n",
    "#         vertex_text='',\n",
    "#         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "#         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "#         output_size=(figsize, figsize),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_all_flows2(g, eps=1e-3, maxiter=1000, use_weights=True):\n",
    "    flows = []\n",
    "    if use_weights:\n",
    "        weight = g.vp.length.a\n",
    "    else:\n",
    "        weight = None\n",
    "    f0 = sp.sparse.csr_array(gt.spectral.adjacency(g))\n",
    "    dd = depth_matrix(g)\n",
    "    for sample_idx in range(g.gp.nsample):\n",
    "        d = dd[sample_idx]\n",
    "        f = estimate_flow(f0, d, weight=weight, eps=eps, maxiter=maxiter)\n",
    "        flows.append(f)\n",
    "    return flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_all_flows2(g[1], use_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 1.\n",
    "g = g[:1]\n",
    "_extracted_seqs = []\n",
    "\n",
    "maxiter = 6\n",
    "for i in range(maxiter):\n",
    "    g.append(g[-1].copy())\n",
    "\n",
    "    mutate_add_flows(g[-1], estimate_all_flows(g[-1], use_weights=True))\n",
    "    mutate_split_all_nodes(g[-1], partial(splits_from_sparse_encoding, threshold=thresh))\n",
    "    # mutate_add_flows(g[-1], estimate_all_flows(g[-1]))\n",
    "\n",
    "    # pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "    # for i in range(3):\n",
    "    #     draw_graph(\n",
    "    #         g[-1],\n",
    "    #         pos=pos,\n",
    "    #         vertex_text='',\n",
    "    #         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "    #         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "    #         output_size=(figsize, figsize),\n",
    "    #     )\n",
    "\n",
    "    mutate_compress_all_unitigs(g[-1])\n",
    "    _, singletons = mutate_extract_singletons(g[-1])\n",
    "    singletons['extraction_round'] = i\n",
    "    _extracted_seqs.append(singletons)\n",
    "    print(i, g[-1])\n",
    "    if g[-1].num_vertices(ignore_filter=True) == 0:\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "    # pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "    # for i in range(3):\n",
    "    #     draw_graph(\n",
    "    #         g[-1],\n",
    "    #         pos=pos,\n",
    "    #         vertex_text='',\n",
    "    #         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "    #         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "    #         output_size=(figsize, figsize),\n",
    "    #     )\n",
    "# _, singletons = mutate_extract_singletons(g[-1])\n",
    "last_extraction = describe_nodes(g[-1])\n",
    "last_extraction['extraction_round'] = -1\n",
    "_extracted_seqs.append(last_extraction)\n",
    "extracted_seqs = pd.concat(_extracted_seqs).reset_index(drop=True)\n",
    "\n",
    "# print((g[0].vp.depth.get_2d_array(pos=[0, 1, 2]) * g[0].vp.length.a).sum(1))\n",
    "# print((g[-1].vp.depth.get_2d_array(pos=[0, 1, 2]) * g[-1].vp.length.a).sum(1) + extracted_seqs[[0, 1, 2]].multiply(extracted_seqs.length, axis=0).sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that we haven't lost any length-x-depth\n",
    "np.asarray((depth_matrix(g[0]) * g[0].vp.length.a).sum(1))[[0, 1, 2]] - extracted_seqs[[0, 1, 2]].multiply(extracted_seqs.length, axis=0).sum(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = extracted_seqs.sort_values('length', ascending=False).head(100).index\n",
    "\n",
    "out = []\n",
    "for query_idx in query_list:\n",
    "    out.append([])\n",
    "    for ref_idx in ref_list:\n",
    "        ref = paths[ref_idx]\n",
    "        query = extracted_seqs.sequence[query_idx]\n",
    "        out[-1].append(edit_ratio(ref, query))\n",
    "out = pd.DataFrame(out, index=query_list, columns=ref_list)\n",
    "\n",
    "out.join(extracted_seqs).sort_values('length', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "node = 2\n",
    "seqs_with_node = idxwhere(extracted_seqs.sequence.apply(lambda x: node in x))\n",
    "extracted_seqs.loc[seqs_with_node].sort_values('length', ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extracted_seqs.sort_values('length', ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calculate edit distances of sequences\n",
    "\n",
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = extracted_seqs.sort_values('length', ascending=False).index\n",
    "\n",
    "ratios = []\n",
    "for query_i in query_list:\n",
    "    ratios.append([])\n",
    "    for ref_j in ref_list:\n",
    "        query = extracted_seqs.sequence[query_i]\n",
    "        ref = paths[ref_j]\n",
    "        ratios[-1].append(edit_ratio(ref, query))\n",
    "edit_score = pd.DataFrame(ratios, columns=ref_list, index=query_list)\n",
    "\n",
    "\n",
    "extracted_seqs.join(edit_score).sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = longest_seqs[:10]\n",
    "\n",
    "fig, axs = plt.subplots(len(query_list), len(ref_list), figsize=(3 * len(ref_list), 3 * len(query_list)), sharex=True, sharey=True)\n",
    "axs = np.asarray(axs).reshape((len(query_list), len(ref_list)))\n",
    "\n",
    "for (i, query_idx), (j, ref_idx), in product(enumerate(query_list), enumerate(ref_list)):\n",
    "    ax = axs[i, j]\n",
    "    ref = paths[ref_idx]\n",
    "    query = _g.vp.sequence[query_idx]\n",
    "    dotplot(query, ref, ax=ax, marker='o', s=1)\n",
    "    ax.annotate(vp.loc[query_idx][ref_idx].round(2), xy=(0.1, 0.9), xycoords='axes fraction', va='top')\n",
    "    ax.annotate(round(edit_ratio(ref, query), 2), xy=(0.7, 0.1), xycoords='axes fraction', va='bottom')\n",
    "        \n",
    "for ref_idx, bottom_row_ax in zip(ref_list, axs[-1,:]):\n",
    "    bottom_row_ax.set_xlabel(ref_idx)\n",
    "    \n",
    "for query_idx, left_column_ax in zip(query_list, axs[:, 0]):\n",
    "    left_column_ax.set_ylabel(query_idx)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "seqs_list = [_g.vp.sequence[i] for i in _g.iter_vertices()]\n",
    "subseq = [9]  # _g.vp.sequence[1235][2:3]\n",
    "\n",
    "acc = np.zeros_like(_g.vp.depth[0])\n",
    "for i, s in enumerate(seqs_list):\n",
    "    if is_subseq(subseq, s):\n",
    "        print(i, len(s), np.asarray(_g.vp.depth[i]).round(3), sep='\\t')\n",
    "        acc += _g.vp.depth[i]\n",
    "print(acc.round(2))\n",
    "print(np.asarray(g[0].vp.depth[subseq[0]]).round(2))\n",
    "              \n",
    "    #           , s)\n",
    "    # if is_subseq(_g.vp.sequence[1235][20:21], s):\n",
    "    #     print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for v in [51, 522, 932, 1520, 1741]:\n",
    "    print(_g.vp.sequence[v][:10], _g.vp.depth[v])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_g = g[0].copy()\n",
    "# Label unitigs\n",
    "labels, counts, _g0_filt = label_maximal_unitigs(_g)\n",
    "\n",
    "list(gt.topology.all_circuits(_g0_filt))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_g = g[0].copy()\n",
    "%prun mutate_compress_all_unitigs(_g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def maximal_unitigs2(g):\n",
    "    \"Generate maximal unitigs as lists of vertices\"\n",
    "    # (1a) Filter edges\n",
    "    # (1b) Label unitigs\n",
    "    labels, counts, g_filt = label_maximal_unitigs2(g)\n",
    "    # (2) Find every node in the filtered graph without in-edges (these are the origins)\n",
    "    is_origin = g_filt.new_vp('bool', g_filt.get_in_degrees(g_filt.get_vertices()) == 0)\n",
    "    # (3) Find every node in the filtered graph without out-edges (these are the termina)\n",
    "    is_terminus = g_filt.new_vp('bool', g_filt.get_out_degrees(g_filt.get_vertices()) == 0)\n",
    "    # (4) Iter through the labels.\n",
    "    for i, c in enumerate(counts):\n",
    "        # (a) For each, select the subgraph for that unitig.\n",
    "        vfilt = (labels.a == i)\n",
    "        assert vfilt.sum() == c\n",
    "        subgraph = gt.GraphView(g, vfilt=vfilt)\n",
    "        # (b) Identify the origin and terminus nodes in the path\n",
    "        vs = subgraph.get_vertices()\n",
    "        # import pdb; pdb.set_trace()\n",
    "        origin = gt.GraphView(subgraph, vfilt=is_origin).get_vertices()\n",
    "        terminus = gt.GraphView(subgraph, vfilt=is_terminus).get_vertices()\n",
    "        if (len(origin) == 0) and (len(terminus) == 0):\n",
    "            origin, terminus = vs[0], vs[0]\n",
    "        elif (len(origin) == 1) and (len(terminus) == 1):\n",
    "            origin, terminus = origin[0], terminus[0]\n",
    "        else:\n",
    "            raise AssertionError(\"If there are multiple origins or termina, then it's not a unitig.\")\n",
    "        # (c) Trace the route from the origin to the terminus (`graph_tool.topology.all_paths`)\n",
    "        unitig = list(gt.topology.all_paths(subgraph, origin, terminus))\n",
    "        if len(unitig) == 0:\n",
    "            assert origin == terminus\n",
    "            continue  # Maximal unitig of length 1. No-op.\n",
    "        assert len(unitig) == 1, \"If there are multiple paths from origin to terminus, then it's not a unitig.\"\n",
    "        unitig = unitig[0]\n",
    "        # (d) Ask if it's a cycle.\n",
    "        is_cycle = is_simple_cycle_unitig(g, vfilt)\n",
    "        if is_cycle and (len(unitig) > 1) and (unitig[0] == unitig[-1]):\n",
    "            unitig = unitig[:-1]\n",
    "            assert len(set(unitig)) == len(unitig)\n",
    "        # (e) Yield the route and whether it's a cycle.\n",
    "        yield list(unitig), is_cycle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def maximal_unitigs3(g):\n",
    "    for i, c in enumerate(counts):\n",
    "        subgraph = gt.GraphView(g_filt, vfilt=(labels.a == i))\n",
    "        if not gt.topology.is_DAG(subgraph):\n",
    "            # It's a cycle, and an independent component, at that.\n",
    "            # Pick an arbitrary vertex as the start\n",
    "            v = next(subgraph.iter_vertices())\n",
    "            # Trace a path from it, to it.\n",
    "            # Should be only one, so we'll only generate one iteration.\n",
    "            vs = list(next(gt.topology.all_paths(subgraph, v, v)))\n",
    "            yield vs[:-1], True\n",
    "        else:\n",
    "            vs = list(gt.topology.topological_sort(subgraph))\n",
    "            # If there's an edge from the last vertex to the\n",
    "            # first in the unfiltered graph, then it's a cycle.\n",
    "            # TODO: Confirm I actually want to report this as\n",
    "            # a cycle. I may have meant that it's an unbroken cycle...\n",
    "            if g.edge(vs[-1], vs[0]):\n",
    "                yield vs, True\n",
    "            else:\n",
    "                yield vs, False\n",
    "\n",
    "%timeit list(maximal_unitigs3(g))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%timeit list(maximal_unitigs2(g))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(maximal_unitigs3(g))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(maximal_unitigs2(g))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pos = draw_graph(g, vertex_fill_color=labels, output_size=(figsize, figsize))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_g = g[0].copy()\n",
    "ut1 = list(maximal_unitigs_old(_g))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_g = g[0].copy()\n",
    "ut2 = list(maximal_unitigs(_g))\n",
    "ut2 = [u for u in ut2 if (len(u[0]) > 1) or u[1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for u1, u2 in zip(ut1, ut2):\n",
    "    assert u1[0] == u2[0]\n",
    "    assert u1[1] == u2[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}