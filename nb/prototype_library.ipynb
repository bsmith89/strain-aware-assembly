{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "_os.chdir(_os.environ['PROJECT_ROOT'])\n",
    "print(_os.path.realpath(_os.path.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lib/saa_prototype.py\n",
    "\n",
    "import graph_tool as gt\n",
    "import graph_tool.draw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import operator\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from sklearn.decomposition import non_negative_factorization\n",
    "from functools import partial\n",
    "import itertools\n",
    "from warnings import warn\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from lib.pandas_util import idxwhere\n",
    "from itertools import starmap\n",
    "import contextlib\n",
    "\n",
    "# Graph generation\n",
    "\n",
    "def path_to_edgelist(path):\n",
    "    u = path[0]\n",
    "    edges = []\n",
    "    for v in path[1:]:\n",
    "        edges.append((u, v))\n",
    "        u = v\n",
    "    return edges\n",
    "\n",
    "\n",
    "def is_subseq(x, y):\n",
    "    # Borrowed from https://stackoverflow.com/a/24017747/1951857\n",
    "    return all(any(c == ch for c in y) for ch in x)\n",
    "\n",
    "\n",
    "def single_stranded_graph_from_merged_paths(paths, lengths, depths):\n",
    "    g = gt.Graph()\n",
    "    all_edges = []\n",
    "    for p in paths:\n",
    "        all_edges.extend(path_to_edgelist(p))\n",
    "    g.add_edge_list(set(all_edges))\n",
    "    g.vp['depth'] = g.new_vp('vector<float>')\n",
    "    g.vp.depth.set_2d_array(depths)\n",
    "    g.vp['length'] = g.new_vp('int', lengths)  \n",
    "    g.gp['nsample'] = g.new_gp('int', len(depths))\n",
    "    g.vp['sequence'] = g.new_vp('object', vals=[[k] for k in range(g.num_vertices())])\n",
    "    g.ep['flow'] = g.new_ep('vector<float>', val=[1] * g.gp.nsample)\n",
    "    return g\n",
    "\n",
    "\n",
    "def single_stranded_graph_with_simulated_depth(paths, depths, length=None, scale_depth_by=1):\n",
    "    if length is None:\n",
    "        length = defaultdict(lambda: 1)\n",
    "\n",
    "    nvertices = max(itertools.chain(*paths.values())) + 1\n",
    "    nsamples = max(len(x) for x in depths.values())\n",
    "    \n",
    "    vertex_length = np.array([length[i] for i in range(nvertices)])\n",
    "\n",
    "    # FIXME: Dimensions seem to break in weird ways when nsamples==1.\n",
    "    # NOTE: Hint: That weirdness DOESN'T happen when I manually set g.gp.nsample=1 after\n",
    "    # graph building. So maybe it's something about nsamples above being set wrong?\n",
    "    expected_depths = np.zeros((nsamples, nvertices))\n",
    "    for p in paths:\n",
    "        expected_depths[:, paths[p]] += np.outer(np.array(depths[p]) * scale_depth_by, np.ones(len(paths[p])))\n",
    "    \n",
    "    # TODO: Consider using nbinom\n",
    "    # # See docs for sp.stats.nbinom\n",
    "    # sigma_sq = expected_depths + dispersion * expected_depths**2\n",
    "    # p = expected_depths / sigma_sq\n",
    "    # n = expected_depths**2 / (sigma_sq - expected_depths)\n",
    "    _depths = sp.stats.poisson(mu=expected_depths).rvs()\n",
    "\n",
    "    g = single_stranded_graph_from_merged_paths(\n",
    "        paths.values(),\n",
    "        depths=_depths,\n",
    "        lengths=vertex_length,\n",
    "    )\n",
    "    return g\n",
    "\n",
    "# Graph statistics\n",
    "\n",
    "def depth_matrix(g, vs=None, samples=None):\n",
    "    if vs is None:\n",
    "        vs = g.get_vertices()\n",
    "    if samples is None:\n",
    "        samples = np.arange(g.gp.nsample)\n",
    "    depth = g.vp.depth.get_2d_array(samples)\n",
    "    return depth[:, vs]\n",
    "\n",
    "\n",
    "def ep_as_adjaceny(ep):\n",
    "    return sp.sparse.csc_array(gt.spectral.adjacency(ep.get_graph(), weight=ep)).toarray()\n",
    "\n",
    "\n",
    "def total_length_x_depth(g):\n",
    "    return (depth_matrix(g) * g.vp.length.a).sum()\n",
    "\n",
    "\n",
    "def edit_ratio(ref, query):\n",
    "    diff = difflib.SequenceMatcher(a=ref, b=query)\n",
    "    return diff.ratio() * (len(diff.a) + len(diff.b)) / (2 * len(diff.b))\n",
    "\n",
    "\n",
    "def vertex_description(g, refs=None):\n",
    "    if refs is None:\n",
    "        refs = []\n",
    "    vertex = pd.DataFrame(dict(\n",
    "        in_degree=g.degree_property_map('in').a,\n",
    "        out_degree=g.degree_property_map('out').a,\n",
    "        length=g.vp.length.a,\n",
    "    ))\n",
    "    depth = pd.DataFrame(depth_matrix(g).T)\n",
    "    depth.rename(columns=lambda i: f\"d{i}\")\n",
    "    return vertex.join(depth)\n",
    "\n",
    "def scale_ep(ep, maximum=2):\n",
    "    g = ep.get_graph()\n",
    "    return g.new_edge_property('float', vals=ep.a * maximum / ep.a.max())\n",
    "\n",
    "def scale_vp(vp, maximum=10):\n",
    "    g = vp.get_graph()\n",
    "    return g.new_vertex_property('float', vals=vp.a * maximum / vp.a.max())\n",
    "\n",
    "# Visualization\n",
    "\n",
    "def draw_graph(g, output_size=(300, 300), ink_scale=0.8, **kwargs):\n",
    "    kwargs = dict(\n",
    "        vertex_fill_color=g.new_vertex_property('float', vals=np.linspace(0, 1, num=max(g.get_vertices()) + 1)),\n",
    "        vertex_text=g.vertex_index,\n",
    "    ) | kwargs\n",
    "    return gt.draw.graph_draw(g, output_size=output_size, ink_scale=ink_scale, **kwargs)\n",
    "\n",
    "\n",
    "def dotplot(pathA, pathB, ax=None, **scatter_kws):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    pathA = np.asanyarray(pathA)\n",
    "    pathB = np.asanyarray(pathB)\n",
    "    length = max(len(pathA), len(pathB)) + 1\n",
    "    pathA = np.pad(pathA, (0, length - len(pathA)), constant_values=-1)\n",
    "    pathA = np.pad(pathA, (0, length - len(pathA)), constant_values=-1)\n",
    "    match = sp.spatial.distance.cdist(pathA.reshape((-1, 1)), pathB.reshape((-1, 1)), metric=lambda x, y: x == y)\n",
    "    ax.scatter(*np.where(match.T), **(dict(marker='o', s=1) | scatter_kws))\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "# Unitigs\n",
    "\n",
    "def edge_has_no_siblings(g):\n",
    "    \"Check whether upstream or downstream sibling edges exist for every edge.\"\n",
    "    vs = g.get_vertices()\n",
    "    v_in_degree = g.degree_property_map('in')\n",
    "    v_out_degree = g.degree_property_map('out')\n",
    "    e_num_in_siblings = gt.edge_endpoint_property(g, v_in_degree, 'target')\n",
    "    e_num_out_siblings = gt.edge_endpoint_property(g, v_out_degree, 'source')\n",
    "    e_has_no_sibling_edges = g.new_edge_property('bool', (e_num_in_siblings.a <= 1) & (e_num_out_siblings.a <= 1))\n",
    "    return e_has_no_sibling_edges\n",
    "\n",
    "\n",
    "def label_maximal_unitigs(g):\n",
    "    \"Assign unitig indices to vertices in maximal unitigs.\"\n",
    "    no_sibling_edges = edge_has_no_siblings(g)\n",
    "    g_filt = gt.GraphView(\n",
    "        g,\n",
    "        efilt=no_sibling_edges,\n",
    "        directed=True\n",
    "    )\n",
    "    labels, counts = gt.topology.label_components(g_filt, directed=False)\n",
    "    return labels, counts, g_filt\n",
    "\n",
    "\n",
    "def maximal_unitigs(g, include_singletons=False):\n",
    "    no_sibling_edges = edge_has_no_siblings(g)\n",
    "    g_no_sibling_edges = gt.GraphView(\n",
    "        g,\n",
    "        efilt=no_sibling_edges,\n",
    "        directed=True\n",
    "    )\n",
    "    \n",
    "    # NOTE: The below is equivalent to\n",
    "    # > isolated_loops = list(gt.topology.all_circuits(g_no_sibling_edges))\n",
    "    labels0, counts0 = gt.topology.label_components(g_no_sibling_edges, directed=False)\n",
    "    in_degree = g_no_sibling_edges.degree_property_map('in').a\n",
    "    out_degree = g_no_sibling_edges.degree_property_map('out').a\n",
    "    all_vs = np.arange(len(labels0.a))\n",
    "    isolated_loops = []\n",
    "    for i, _ in enumerate(counts0):\n",
    "        unitig_mask = labels0.a == i\n",
    "        if (in_degree[unitig_mask] == 1).all() and (out_degree[unitig_mask] == 1).all():\n",
    "            isolated_loops.append(all_vs[unitig_mask])\n",
    "    \n",
    "    out = []\n",
    "    _mark_isolated_loops = []\n",
    "    for vs in isolated_loops:\n",
    "        vs = list(vs)\n",
    "        if include_singletons or (len(vs) > 1):\n",
    "            out.append((vs, True))\n",
    "        _mark_isolated_loops.extend(vs)\n",
    "    isolated_loop_mask = g_no_sibling_edges.new_vertex_property('bool', val=1)\n",
    "    isolated_loop_mask.a[_mark_isolated_loops] = 0\n",
    "    \n",
    "    g_no_siblings_no_loops = gt.GraphView(\n",
    "        g_no_sibling_edges,\n",
    "        vfilt=isolated_loop_mask,\n",
    "        directed=True\n",
    "    )\n",
    "    labels1, counts1 = gt.topology.label_components(g_no_siblings_no_loops, directed=False)\n",
    "    tsort_idx = gt.topology.topological_sort(g_no_siblings_no_loops)\n",
    "    tsort_labels = labels1.a[tsort_idx]\n",
    "    for i, c in enumerate(counts1):\n",
    "        if (c == 1) and not include_singletons:\n",
    "            continue\n",
    "        vs = list(tsort_idx[tsort_labels == i])\n",
    "        if g.edge(vs[-1], vs[0]):\n",
    "            out.append((vs, True))\n",
    "        else:\n",
    "            out.append((vs, False))\n",
    "    return out\n",
    "\n",
    "\n",
    "def list_unitig_neighbors(g, vs):\n",
    "    \"The in and out neighbors of a unitig path.\"\n",
    "    all_ins = set(g.get_in_neighbors(vs[0]))\n",
    "    all_outs = set(g.get_out_neighbors(vs[-1]))\n",
    "    return list(set(all_ins) - set(vs)), list(set(all_outs) - set(vs))\n",
    "\n",
    "\n",
    "def mutate_add_compressed_unitig_vertex(g, vs, is_cycle, old_depths=None, drop_vs=False):\n",
    "    if old_depths is None:\n",
    "        old_depths = depth_matrix(g, vs)\n",
    "    v = int(g.add_vertex())\n",
    "    nsample = g.gp.nsample\n",
    "    in_neighbors, out_neighbors = list_unitig_neighbors(g, vs)\n",
    "    g.add_edge_list((neighbor, v) for neighbor in in_neighbors)\n",
    "    g.add_edge_list((v, neighbor) for neighbor in out_neighbors)\n",
    "    g.vp.length.a[v] = g.vp.length.a[vs].sum()\n",
    "    g.vp.sequence[v] = reduce(operator.add, (g.vp.sequence[u] for u in vs), [])\n",
    "    new_depth = (\n",
    "        (\n",
    "            old_depths\n",
    "            * g.vp.length.a[vs]\n",
    "        ).sum(1) / g.vp.length.a[v]\n",
    "    )\n",
    "    g.vp.depth[v] = new_depth\n",
    "    # FIXME: Consider dropping this assert.\n",
    "    assert np.allclose(\n",
    "        (old_depths * g.vp.length.a[vs]).sum(1),\n",
    "        # FIXME: Does new_depth need to be reshaped before multiplication?\n",
    "        (new_depth.reshape((-1, 1)) * g.vp.length.a[v]).sum(1)\n",
    "    )\n",
    "    if is_cycle:\n",
    "        g.add_edge(v, v)\n",
    "    for old_v in vs:\n",
    "        g.clear_vertex(old_v)\n",
    "    return g\n",
    "\n",
    "\n",
    "def mutate_compress_all_unitigs(g):\n",
    "    unitig_list = maximal_unitigs(g, include_singletons=False)\n",
    "    all_vs = []\n",
    "    old_depths = depth_matrix(g)\n",
    "    for i, (vs, is_cycle) in enumerate(unitig_list):\n",
    "        assert len(vs) > 1  # All len(vs) == 1 should be removed by include_singletons=False.\n",
    "        mutate_add_compressed_unitig_vertex(g, vs, is_cycle, old_depths=old_depths[:, vs])\n",
    "        all_vs.extend(vs)\n",
    "    g.remove_vertex(set(all_vs), fast=True)\n",
    "    # I think, but am not sure, that the number of nodes removed will always equal the number of edges removed.\n",
    "    return g\n",
    "\n",
    "\n",
    "def label_self_looping_vertices(g):\n",
    "    return (\n",
    "        gt.incident_edges_op(\n",
    "            g,\n",
    "            direction='in',\n",
    "            op='max',\n",
    "            eprop=gt.topology.label_self_loops(g, mark_only=True),\n",
    "        )\n",
    "    )\n",
    "\n",
    "def describe_nodes(g):\n",
    "    description = pd.DataFrame(dict(\n",
    "        length=g.vp.length.a,\n",
    "        in_degree=g.degree_property_map('in').a,\n",
    "        out_degree=g.degree_property_map('out').a,\n",
    "        circular=label_self_looping_vertices(g).a,\n",
    "        sequence=[g.vp.sequence[v] for v in g.iter_vertices()],\n",
    "    ))\n",
    "    depth = pd.DataFrame(depth_matrix(g).T)\n",
    "    return description.join(depth)\n",
    "\n",
    "\n",
    "def mutate_extract_singletons(g):\n",
    "    nodes = describe_nodes(g)\n",
    "    singletons = idxwhere((nodes.in_degree + nodes.out_degree - 2 * nodes.circular) == 0)\n",
    "    g.remove_vertex(singletons, fast=True)\n",
    "    return g, nodes.loc[singletons].reset_index(drop=True)\n",
    "\n",
    "# Flows\n",
    "\n",
    "def estimate_flow_old(f0, d, weight=None, eps=1e-2, maxiter=100):\n",
    "    if weight is None:\n",
    "        weight = np.ones_like(d)\n",
    "\n",
    "    loss_hist = [np.finfo('float').max]\n",
    "    f = f0\n",
    "    for step_i in range(maxiter):\n",
    "        f_out = f\n",
    "        f_total_out = f_out.sum(1)\n",
    "        d_error_out = f_total_out - d\n",
    "        \n",
    "        f_in = f_out.T\n",
    "        f_total_in = f_in.sum(1)\n",
    "        d_error_in = f_total_in - d\n",
    "        \n",
    "        loss_hist.append(np.square(d_error_out).sum() + np.square(d_error_in).sum())\n",
    "        if loss_hist[-1] == 0:\n",
    "            break  # This should only happen if d is all 0's.\n",
    "        loss_ratio = (loss_hist[-2] - loss_hist[-1]) / loss_hist[-2]\n",
    "        if loss_ratio < eps:\n",
    "            break\n",
    "            \n",
    "        # NOTE: Because of errstate, this function is NOT threadsafe.\n",
    "        # TODO: Determine if it's safe across multiple processes.\n",
    "        with np.errstate(divide='ignore', over='ignore'):\n",
    "            allocation_out = f_out.T * np.nan_to_num(1 / f_total_out, posinf=1, nan=0)\n",
    "        allocated_d_error_out = (allocation_out * d_error_out).T\n",
    "        with np.errstate(divide='ignore', over='ignore'):\n",
    "            allocation_in = f_in.T * np.nan_to_num(1 / f_total_in, posinf=1, nan=0)\n",
    "        allocated_d_error_in = (allocation_in * d_error_in).T\n",
    "\n",
    "        # The final step is calculated as a average of the in and out error, weighted\n",
    "        # by the node weight.\n",
    "        inv_weight = 1 / weight\n",
    "        mean_allocated_d_error = (\n",
    "            ((allocated_d_error_in * inv_weight).T + (allocated_d_error_out * inv_weight))\n",
    "            * (1 / (inv_weight.reshape((-1, 1)) + (inv_weight.reshape((1, -1)))))\n",
    "        )\n",
    "        \n",
    "        f = (f_out - mean_allocated_d_error)\n",
    "        # Very rarely floating point precision results in very small, negative values for f.\n",
    "        assert f.min() >= -1e-20\n",
    "        # Replace these with 0.\n",
    "        f[f < 0] = 0\n",
    "    else:\n",
    "        warn(f\"loss_ratio < eps ({eps}) not achieved in maxiter ({maxiter}) steps. Final loss_ratio={loss_ratio}. Final loss={loss_hist[-1]}.\")\n",
    "    return f\n",
    "\n",
    "\n",
    "# This is used in the parallel implementation of estimate_all_flows.\n",
    "def _estimate_flow_old(kwargs):\n",
    "    return estimate_flow_old(**kwargs)\n",
    "\n",
    "\n",
    "def estimate_all_flows_old(g, eps=1e-3, maxiter=1000, use_weights=True, jobs=1):\n",
    "    if jobs != 1:\n",
    "        pool = Pool(jobs)\n",
    "        map_f = pool.map\n",
    "    else:\n",
    "        map_f = lambda *args, **kwargs: list(map(*args, **kwargs))\n",
    "    flows = []\n",
    "    if use_weights:\n",
    "        weight = g.vp.length.a\n",
    "    else:\n",
    "        weight = None\n",
    "    f0 = sp.sparse.csr_array(gt.spectral.adjacency(g))\n",
    "    dd = depth_matrix(g)\n",
    "    flows = map_f(\n",
    "        _estimate_flow_old,\n",
    "        [\n",
    "            dict(\n",
    "                f0=f0, d=dd[sample_idx], weight=weight, eps=eps, maxiter=maxiter\n",
    "            )\n",
    "            for sample_idx in range(g.gp.nsample)\n",
    "        ],\n",
    "    )\n",
    "    return flows\n",
    "\n",
    "\n",
    "def mutate_add_flows_old(g, flows):\n",
    "    props = []\n",
    "    for sample_idx, f in enumerate(flows):\n",
    "        p = g.new_edge_property('float', val=0)\n",
    "        for i, j in g.get_edges():\n",
    "            p[g.edge(i, j)] = f[j, i]\n",
    "        props.append(p)\n",
    "    props = gt.group_vector_property(props)\n",
    "    g.ep['flow'] = props\n",
    "    return g\n",
    "\n",
    "\n",
    "def estimate_all_flows(g, eps=0.001, maxiter=1000, use_weights=True):\n",
    "    if use_weights:\n",
    "        weight = g.vp.length\n",
    "    else:\n",
    "        weight = g.new_vertex_property('float', val=1)\n",
    "    target_vertex_weight = gt.edge_endpoint_property(g, weight, 'target')\n",
    "    source_vertex_weight = gt.edge_endpoint_property(g, weight, 'source')\n",
    "    all_flows = []\n",
    "    for i in range(g.gp.nsample):\n",
    "        depth = gt.ungroup_vector_property(g.vp.depth, [i])[0]\n",
    "        flow = g.new_edge_property('float', val=1)\n",
    "        flow.a[:] = 1\n",
    "        loss_hist = [np.finfo('float').max]\n",
    "        for _ in range(maxiter):\n",
    "            total_in_flow = gt.incident_edges_op(g, 'in', 'sum', flow)\n",
    "            in_flow_error = g.new_vertex_property('float', vals=depth.a - total_in_flow.a)\n",
    "            target_vertex_total_inflow = gt.edge_endpoint_property(g, total_in_flow, 'target')\n",
    "            target_vertex_error = gt.edge_endpoint_property(g, in_flow_error, 'target')\n",
    "            with np.errstate(divide='ignore', over='ignore', invalid='ignore'):\n",
    "                target_vertex_alloc = np.nan_to_num(flow.a / target_vertex_total_inflow.a, posinf=1, nan=0)\n",
    "            target_vertex_alloc_error = target_vertex_alloc * target_vertex_error.a\n",
    "\n",
    "            total_out_flow = gt.incident_edges_op(g, 'out', 'sum', flow)\n",
    "            out_flow_error = g.new_vertex_property('float', vals=depth.a - total_out_flow.a)\n",
    "            source_vertex_total_outflow = gt.edge_endpoint_property(g, total_out_flow, 'source')\n",
    "            source_vertex_error = gt.edge_endpoint_property(g, out_flow_error, 'source')\n",
    "            with np.errstate(divide='ignore', over='ignore', invalid='ignore'):\n",
    "                source_vertex_alloc = np.nan_to_num(flow.a / source_vertex_total_outflow.a, posinf=1, nan=0)\n",
    "            source_vertex_alloc_error = source_vertex_alloc * source_vertex_error.a\n",
    "\n",
    "            loss_hist.append(np.square(in_flow_error.a).sum() + np.square(out_flow_error.a).sum())\n",
    "            if loss_hist[-1] == 0:\n",
    "                break  # This should only happen if d is all 0's.\n",
    "            loss_ratio = (loss_hist[-2] - loss_hist[-1]) / loss_hist[-2]\n",
    "            if loss_ratio < eps:\n",
    "                break\n",
    "            \n",
    "            # FIXME: Catch one of the \"invalid value\" warnings and see\n",
    "            # what's causing it.\n",
    "            mean_flow_error = g.new_edge_property(\n",
    "                'float',\n",
    "                vals=(\n",
    "                    (source_vertex_alloc_error * source_vertex_weight.a)\n",
    "                    +\n",
    "                    (target_vertex_alloc_error * target_vertex_weight.a)\n",
    "                )\n",
    "                / (source_vertex_weight.a + target_vertex_weight.a)\n",
    "            )\n",
    "            # NOTE: Having asserted np.isfinite(a).all() for all arrays in the expression\n",
    "            # above, I'm sure I don't know what's causing the\n",
    "            # \"RuntimeWarning: invalid value encountered in divide\" I'm getting.\n",
    "            flow = g.new_edge_property('float', vals=flow.a + mean_flow_error.a)\n",
    "        all_flows.append(flow)\n",
    "    return all_flows\n",
    "\n",
    "\n",
    "def mutate_add_flows(g, flows):\n",
    "    g.ep['flow'] = gt.group_vector_property(flows)\n",
    "\n",
    "# Node splitting\n",
    "\n",
    "Split = namedtuple('Split', ['u', 'v', 'w'])\n",
    "\n",
    "\n",
    "def splits_from_sparse_encoding(g, v, threshold=1.):\n",
    "    # Compile tables\n",
    "    in_neighbors = list(sorted(g.get_in_neighbors(v)))\n",
    "    num_in_neighbors = len(in_neighbors)\n",
    "    in_neighbors_label = {k: v for k, v in enumerate(in_neighbors)}\n",
    "    in_neighbors_onehot = {k: v for k, v in zip(in_neighbors, np.eye(num_in_neighbors))}\n",
    "    in_neighbors_onehot[None] = np.zeros(num_in_neighbors)\n",
    "\n",
    "    out_neighbors = list(sorted(g.get_out_neighbors(v)))\n",
    "    num_out_neighbors = len(out_neighbors)\n",
    "    out_neighbors_label = {k: v for k, v in enumerate(out_neighbors)}\n",
    "    out_neighbors_onehot = {k: v for k, v in zip(out_neighbors, np.eye(num_out_neighbors))}\n",
    "    out_neighbors_onehot[None] = np.zeros(num_out_neighbors)\n",
    "\n",
    "    # Build observation matrix.\n",
    "    in_neighbor_flow = []\n",
    "    for u in in_neighbors:\n",
    "        in_neighbor_flow.append(g.ep.flow[g.edge(u, v)])\n",
    "    out_neighbor_flow = []\n",
    "    for w in out_neighbors:\n",
    "        out_neighbor_flow.append(g.ep.flow[g.edge(v, w)])\n",
    "    depth_row = g.vp.depth[v].a\n",
    "    obs = np.stack(in_neighbor_flow + [depth_row] + out_neighbor_flow).T\n",
    "\n",
    "    # Build code matrix.\n",
    "    in_neighbor_code = []\n",
    "    out_neighbor_code = []\n",
    "    split_idx = {}\n",
    "    for i, (u, w) in enumerate(product(in_neighbors + [None], out_neighbors + [None])):\n",
    "        if (u, w) == (None, None):\n",
    "            # NOTE: I treat the naked vertex specially.\n",
    "            naked_vertex_idx = i\n",
    "            pass\n",
    "        in_neighbor_code.append(in_neighbors_onehot[u])\n",
    "        out_neighbor_code.append(out_neighbors_onehot[w])\n",
    "        split_idx[i] = (u, w)\n",
    "    in_neighbor_code = np.stack(in_neighbor_code)\n",
    "    out_neighbor_code = np.stack(out_neighbor_code)\n",
    "    unnormalized_code = np.concatenate([\n",
    "        in_neighbor_code,\n",
    "        np.ones((in_neighbor_code.shape[0], 1)),\n",
    "        out_neighbor_code\n",
    "    ], axis=1)\n",
    "    code_magnitude = np.sqrt(np.square(unnormalized_code).sum(1, keepdims=True))\n",
    "    code = unnormalized_code / code_magnitude\n",
    "\n",
    "    # Group Matching Pursuit (GMP)\n",
    "    # Inspired by https://arxiv.org/pdf/1812.10538.pdf\n",
    "    resid = obs\n",
    "    atoms = []\n",
    "    dictionary = np.zeros_like(code)\n",
    "    normalized_encoding = np.zeros((obs.shape[0], dictionary.shape[0]))\n",
    "    for _ in range(code.shape[0]):\n",
    "        loss = np.abs(resid).sum()\n",
    "        dot = resid @ code.T\n",
    "        # TODO: Decide how to decide atoms\n",
    "        # next_atom = np.square(dot).sum(0).argmax()\n",
    "        next_atom = dot.argmax() % code.shape[0]\n",
    "        naked_vertex_dot = dot[:, naked_vertex_idx]\n",
    "        if dot[:, next_atom].sum() <= threshold:\n",
    "            break\n",
    "        if next_atom in atoms:\n",
    "            break\n",
    "        if next_atom == naked_vertex_idx:\n",
    "            break  # TODO: Decide if this is a good stopping criterion.\n",
    "        atoms.append(next_atom)\n",
    "        dictionary[atoms[-1]] = code[atoms[-1]]\n",
    "        normalized_encoding, _, _ = non_negative_factorization(obs, n_components=dictionary.shape[0], H=dictionary, update_H=False, alpha_W=0)\n",
    "        resid = obs - normalized_encoding @ code\n",
    "        \n",
    "\n",
    "    # Iterate through selected atoms as splits.\n",
    "    encoding = normalized_encoding / code_magnitude.T\n",
    "    out = []\n",
    "    for i in atoms:\n",
    "        if i == naked_vertex_idx:\n",
    "            # Don't return the naked vertex here. It'll be returned\n",
    "            # later.\n",
    "            continue\n",
    "        u, w = split_idx[i]\n",
    "        out.append((Split(u, v, w), g.vp.length[v], encoding[:, i]))\n",
    "    # Remaining depth must also include any depth assigned to the naked encoding.\n",
    "    remaining_depth = g.vp.depth[v] - encoding.sum(1) + encoding[:, naked_vertex_idx]\n",
    "    if not np.allclose(remaining_depth, 0):\n",
    "        out.append((Split(None, v, None), g.vp.length[v], remaining_depth))\n",
    "    return out\n",
    "        \n",
    "        \n",
    "def build_tables_from_splits(split_list, start_idx):\n",
    "    \"\"\"Generate edges to and from new, split vertices.\n",
    "    \n",
    "    Note that if splits from adjacent parents are not\n",
    "    reciprocated, no new edge is produced.\n",
    "    \n",
    "    \"\"\"\n",
    "    split_idx = {}\n",
    "    upstream = defaultdict(list)\n",
    "    downstream = defaultdict(list)\n",
    "    length = []\n",
    "    depth = []\n",
    "    for idx, (split, l, d) in enumerate(split_list, start=start_idx):\n",
    "        u, v, w = split\n",
    "        split_idx[split] = idx\n",
    "        upstream[(v, w)].append(split)\n",
    "        downstream[(u, v)].append(split)\n",
    "        depth.append(d)\n",
    "        length.append(l)\n",
    "    assert len(split_list) == len(split_idx)\n",
    "    return split_idx, upstream, downstream, np.array(length), np.array(depth)\n",
    "        \n",
    "        \n",
    "def new_edges_from_splits(split_list, split_idx, upstream, downstream, start_idx):\n",
    "    out = []\n",
    "    for v, (split, _, _) in enumerate(split_list, start=start_idx):\n",
    "        u_old, v_old, w_old = split\n",
    "        v = split_idx[split]\n",
    "        \n",
    "        # Upstream edges\n",
    "        if u_old is not None:\n",
    "            out.append((u_old, v))\n",
    "        for upstream_split in upstream[(u_old, v_old)]:\n",
    "            u = split_idx[upstream_split]\n",
    "            if u is not None:\n",
    "                out.append((u, v))\n",
    "            \n",
    "        # Downstream edges\n",
    "        if w_old is not None:\n",
    "            out.append((v, w_old))\n",
    "        for downstream_split in downstream[(v_old, w_old)]:\n",
    "            w = split_idx[downstream_split]\n",
    "            if w is not None:\n",
    "                out.append((v, w))\n",
    "    return out\n",
    "            \n",
    "            \n",
    "def mutate_apply_splits(g, split_list):\n",
    "    \"\"\"Add edges and drop any parent vertices that were split.\n",
    "    \n",
    "    \"\"\"\n",
    "    start_idx = g.num_vertices(ignore_filter=True)\n",
    "    split_idx, upstream, downstream, lengths, depths = (\n",
    "        build_tables_from_splits(split_list, start_idx=start_idx)\n",
    "    )\n",
    "    edges_to_add = list(set(new_edges_from_splits(\n",
    "        split_list, split_idx, upstream, downstream, start_idx\n",
    "    )))\n",
    "    \n",
    "    # NOTE: Without adding vertices before edges I can get an IndexError\n",
    "    # running `g.vp.length.a[np.arange(len(lengths)) + start_idx]`.\n",
    "    # I believe this is because one or more split nodes\n",
    "    # are without any new edges, and therefore these nodes don't\n",
    "    # get added implicitly by `g.add_edge_list`.\n",
    "    # When these accidentally hidden nodes are the highest valued\n",
    "    # ones, they also don't get implicitly added due to their index.\n",
    "    # The result is that I'm missing nodes that should actually exist.\n",
    "    # NOTE: This line returns an unassigned generator. I _think_ all the\n",
    "    # nodes are still added, but it's not entirely clear.\n",
    "    # UPDATE: I'm sure the nodes are still added because of the\n",
    "    # assert afterwards.\n",
    "    g.add_vertex(n=max(split_idx.values()) - max(g.get_vertices()))\n",
    "    g.add_edge_list(set(edges_to_add))\n",
    "    assert max(g.get_vertices()) == max(split_idx.values())\n",
    "    \n",
    "    g.vp.length.a[np.arange(len(lengths)) + start_idx] = lengths\n",
    "    new_depth = depth_matrix(g)\n",
    "    new_depth[:, np.arange(len(depths)) + start_idx] = depths.T\n",
    "    g.vp.depth.set_2d_array(new_depth)\n",
    "    for split, _, _ in split_list:\n",
    "        g.vp.sequence[split_idx[split]] = g.vp.sequence[split.v]\n",
    "    # for v in g.iter_vertices():\n",
    "    #     assert g.vp.sequence[v]\n",
    "    vertices_to_drop = set(split.v for (split, _, _) in split_list)\n",
    "    g.remove_vertex(vertices_to_drop, fast=True)\n",
    "    return g\n",
    "\n",
    "\n",
    "def splits_for_all_vertices_old(g, split_func):\n",
    "    out = []\n",
    "    for v in g.vertices():\n",
    "        if (v.in_degree() < 2) and (v.out_degree() < 2):\n",
    "            continue\n",
    "        else:\n",
    "            out.extend(split_func(g, v))\n",
    "    return out\n",
    "\n",
    "\n",
    "def splits_for_all_vertices(g, split_func, jobs=1):\n",
    "    if jobs != 1:\n",
    "        pool = Pool(jobs)\n",
    "        map_f = pool.starmap\n",
    "    else:\n",
    "        map_f = lambda *args, **kwargs: list(starmap(*args, **kwargs))\n",
    "        \n",
    "    # NOTE: I _think_ this is why my splits sometimes have integer u and w,\n",
    "    # but vertex(v).\n",
    "    # TODO: Consider using int(v) instead of v, for consistency with other\n",
    "    # code.\n",
    "    non_linear_vs = [int(v) for v in g.vertices() if (v.in_degree() >= 2) or (v.out_degree() >= 2)]\n",
    "    splits = map_f(split_func, [(g, v) for v in non_linear_vs])\n",
    "    return list(itertools.chain.from_iterable(splits))\n",
    "            \n",
    "\n",
    "def mutate_split_all_nodes(g, split_func):\n",
    "    # TODO: Vertices with <= 1 local path will be split into just\n",
    "    # themselves pointing trivially at their neighbors.\n",
    "    # These can be dropped as they are effectively a no-op.\n",
    "    split_list = list(splits_for_all_vertices(g, split_func))\n",
    "    if len(split_list) > 0:\n",
    "        g = mutate_apply_splits(g, split_list=split_list)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.saa_prototype import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "gt.seed_rng(2)\n",
    "\n",
    "paths = [\n",
    "    [0, 2, 3],\n",
    "    [1, 2, 4],\n",
    "    # [1, 3, 4, 5],\n",
    "    # [0, 0],\n",
    "]\n",
    "\n",
    "nnodes = max(itertools.chain(*paths)) + 1\n",
    "\n",
    "depths = np.array([\n",
    "    [4, 4, 4, 4, 4],\n",
    "])\n",
    "nsamples = depths.shape[0]\n",
    "\n",
    "_g = single_stranded_graph_from_merged_paths(\n",
    "    paths,\n",
    "    depths=depths,\n",
    "    lengths=[1, 1, 3, 1, 1],\n",
    "    # lengths=np.array([1] * nnodes),\n",
    ")\n",
    "\n",
    "figsize=150\n",
    "pos = draw_graph(_g, output_size=(figsize, figsize))\n",
    "draw_graph(_g, pos=pos, vertex_text=_g.vp.length, output_size=(figsize, figsize))\n",
    "\n",
    "f0 = sp.sparse.csr_array(gt.spectral.adjacency(_g))#.toarray()\n",
    "d = _g.vp.depth.get_2d_array([0])[0]\n",
    "# estimate_flow3(f0, d, eps=1e-2, maxiter=1000, weight=_g.vp.length.a).round(3)\n",
    "# draw_graph(\n",
    "#     _g,\n",
    "#     pos=pos,\n",
    "#     vertex_text=gt.ungroup_vector_property(_g.vp.depth, pos=[0])[0],\n",
    "#     edge_pen_width=gt.ungroup_vector_property(_g.ep.flow, pos=[0])[0],\n",
    "# )\n",
    "\n",
    "mutate_add_flows(_g, estimate_all_flows(_g))\n",
    "draw_graph(\n",
    "    _g,\n",
    "    pos=pos,\n",
    "    vertex_text=gt.ungroup_vector_property(_g.vp.depth, pos=[0])[0],\n",
    "    edge_pen_width=gt.ungroup_vector_property(_g.ep.flow, pos=[0])[0],\n",
    "    output_size=(figsize, figsize),\n",
    ")\n",
    "# mutate_add_flows(_g, estimate_all_flows3(_g))\n",
    "# draw_graph(\n",
    "#     _g,\n",
    "#     pos=pos,\n",
    "#     vertex_text=gt.ungroup_vector_property(_g.vp.depth, pos=[0])[0],\n",
    "#     edge_pen_width=gt.ungroup_vector_property(_g.ep.flow, pos=[0])[0],\n",
    "#     output_size=(figsize, figsize),\n",
    "# )\n",
    "ep_as_adjaceny(estimate_all_flows(_g)[0]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "gt.seed_rng(1)\n",
    "\n",
    "paths = dict(\n",
    "    a=[0, 1, 2, 3, 0],\n",
    "    b=[4, 5, 6, 7, 4],\n",
    "    c=[0, 4, 8, 9],\n",
    "    d=[10, 11, 12, 10],\n",
    "    e=[4, 4],\n",
    ")\n",
    "\n",
    "mean_depths = dict(\n",
    "    a=[1, 2],\n",
    "    b=[1, 2],\n",
    "    c=[1, 2],\n",
    "    d=[1, 2],\n",
    "    e=[1, 2],\n",
    ")\n",
    "\n",
    "figsize = 200\n",
    "\n",
    "_g = single_stranded_graph_with_simulated_depth(\n",
    "        paths,\n",
    "        depths=mean_depths, scale_depth_by=1,\n",
    ")\n",
    "\n",
    "labels, counts, _g_filt = label_maximal_unitigs(_g)\n",
    "\n",
    "pos = draw_graph(_g, vertex_fill_color=labels, output_size=(figsize, figsize))\n",
    "draw_graph(_g_filt, pos=pos, vertex_fill_color=labels, vertex_text=labels, output_size=(figsize, figsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_g = single_stranded_graph_from_merged_paths(\n",
    "    paths=[[0, 1, 2, 0], [3, 4, 5, 3], [3, 5], [4, 3], [6, 7, 6]],\n",
    "    depths=np.array([[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]),\n",
    "    lengths=np.array([1, 1, 1, 1, 1, 1, 1, 1]),\n",
    ")\n",
    "mutate_add_flows(_g, estimate_all_flows(_g))\n",
    "i = 0\n",
    "draw_graph(\n",
    "    _g,\n",
    "    # pos=pos,\n",
    "    # vertex_text='',\n",
    "    # vertex_size=scale_vp(gt.ungroup_vector_property(_g.vp.depth, pos=[i])[0], maximum=20),\n",
    "    # edge_pen_width=scale_ep(gt.ungroup_vector_property(_g.ep.flow, pos=[i])[0], maximum=8),\n",
    "    output_size=(figsize, figsize),\n",
    ")\n",
    "ep_as_adjaceny(gt.ungroup_vector_property(_g.ep.flow, pos=[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f maximal_unitigs ut0 = maximal_unitigs(_g, include_singletons=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from functools import partial\n",
    "\n",
    "np.random.seed(1)\n",
    "gt.seed_rng(1)\n",
    "\n",
    "strain_length = 20000\n",
    "num_mutations = strain_length // 2\n",
    "ancestral_path = np.arange(strain_length)\n",
    "\n",
    "strainA_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainA_path = ancestral_path.copy()\n",
    "strainA_path[strainA_mutations] = np.arange(strain_length + 0 * num_mutations, strain_length + 1 * num_mutations)\n",
    "\n",
    "strainB_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainB_path = ancestral_path.copy()\n",
    "strainB_path[strainB_mutations] = np.arange(strain_length + 1 * num_mutations, strain_length + 2 * num_mutations)\n",
    "\n",
    "strainC_mutations = np.random.choice(ancestral_path, size=num_mutations, replace=False)\n",
    "strainC_path = ancestral_path.copy()\n",
    "strainC_path[strainC_mutations] = np.arange(strain_length + 2 * num_mutations, strain_length + 3 * num_mutations)\n",
    "\n",
    "num_errors = strain_length // 10\n",
    "error_path = np.arange(strain_length + 3 * num_mutations, strain_length + 3 * num_mutations + num_errors, step=0.5)\n",
    "error_path[1::2] = np.random.choice(np.arange(strain_length + 3 * num_mutations), size=num_errors)\n",
    "error_path = list(error_path.astype(int))\n",
    "\n",
    "paths = dict(\n",
    "    x=ancestral_path,\n",
    "    a=strainA_path,\n",
    "    b=strainB_path,\n",
    "    c=strainC_path,\n",
    "    error=error_path,\n",
    "    # self_looping=[10, 500, 10],\n",
    ")\n",
    "\n",
    "sample_replicates = 1\n",
    "mean_depths = dict(\n",
    "    a=[0.90, 0.05, 0.05] * sample_replicates,\n",
    "    b=[0.05, 0.90, 0.05] * sample_replicates,\n",
    "    c=[0.05, 0.05, 0.90] * sample_replicates,\n",
    "    x=[0.00, 0.00, 0.00] * sample_replicates,\n",
    "    error=[0.01, 0.01, 0.01] * sample_replicates,\n",
    "    # self_looping=[1, 1, 1] * sample_replicates,\n",
    ")\n",
    "\n",
    "g = []\n",
    "g.append(\n",
    "    single_stranded_graph_with_simulated_depth(\n",
    "        paths,\n",
    "        depths=mean_depths, scale_depth_by=20,\n",
    "))\n",
    "\n",
    "figsize = 250\n",
    "# draw_graph(g0, output_size=(figsize, figsize))\n",
    "# g1 = mutate_compress_all_unitigs(g0.copy())\n",
    "# mutate_add_flows(g[-1], estimate_all_flows(g[-1]))\n",
    "\n",
    "# pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "# for i in range(3):\n",
    "#     draw_graph(\n",
    "#         g[-1],\n",
    "#         pos=pos,\n",
    "#         vertex_text='',\n",
    "#         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "#         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "#         output_size=(figsize, figsize),\n",
    "#     )\n",
    "    \n",
    "g.append(g[-1].copy())\n",
    "%time mutate_compress_all_unitigs(g[-1])\n",
    "# mutate_add_flows(g[-1], estimate_all_flows(g[-1]))\n",
    "# pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "# for i in range(3):\n",
    "#     draw_graph(\n",
    "#         g[-1],\n",
    "#         pos=pos,\n",
    "#         vertex_text='',\n",
    "#         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "#         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "#         output_size=(figsize, figsize),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%prun\n",
    "\n",
    "thresh = 1\n",
    "_g = g[1].copy()\n",
    "%time mutate_add_flows(_g, estimate_all_flows(_g))\n",
    "%time mutate_split_all_nodes(_g, partial(splits_from_sparse_encoding, threshold=thresh))\n",
    "# %time mutate_compress_all_unitigs(_g)\n",
    "# %time _, singletons = mutate_extract_singletons(_g)\n",
    "\n",
    "_g0 = _g.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f mutate_compress_all_unitigs mutate_compress_all_unitigs(_g0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%prun\n",
    "\n",
    "thresh = 1\n",
    "g = g[:2]\n",
    "_extracted_seqs = []\n",
    "\n",
    "maxiter = 10\n",
    "for i in range(maxiter):\n",
    "    g.append(g[-1].copy())\n",
    "\n",
    "    %time mutate_add_flows(g[-1], estimate_all_flows(g[-1], use_weights=True))\n",
    "    %time mutate_split_all_nodes(g[-1], partial(splits_from_sparse_encoding, threshold=thresh))\n",
    "    # mutate_add_flows(g[-1], estimate_all_flows(g[-1]))\n",
    "\n",
    "    # pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "    # for i in range(3):\n",
    "    #     draw_graph(\n",
    "    #         g[-1],\n",
    "    #         pos=pos,\n",
    "    #         vertex_text='',\n",
    "    #         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "    #         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "    #         output_size=(figsize, figsize),\n",
    "    #     )\n",
    "\n",
    "    %time mutate_compress_all_unitigs(g[-1])\n",
    "    _, singletons = mutate_extract_singletons(g[-1])\n",
    "    singletons['extraction_round'] = i\n",
    "    _extracted_seqs.append(singletons)\n",
    "    print(i, g[-1])\n",
    "    if g[-1].num_vertices(ignore_filter=True) == 0:\n",
    "        print(\"DONE: No more vertices.\")\n",
    "        break\n",
    "    if (g[-1].num_edges() == g[-2].num_edges()) & (g[-1].num_vertices() == g[-2].num_vertices()):\n",
    "        print(\"DONE: Same number of edges and vertices.\")\n",
    "        break\n",
    "    # pos = draw_graph(g[-1], output_size=(figsize, figsize))\n",
    "    # for i in range(3):\n",
    "    #     draw_graph(\n",
    "    #         g[-1],\n",
    "    #         pos=pos,\n",
    "    #         vertex_text='',\n",
    "    #         vertex_size=scale_vp(gt.ungroup_vector_property(g[-1].vp.depth, pos=[i])[0]),\n",
    "    #         edge_pen_width=scale_flow_ep(gt.ungroup_vector_property(g[-1].ep.flow, pos=[i])[0]),\n",
    "    #         output_size=(figsize, figsize),\n",
    "    #     )\n",
    "# _, singletons = mutate_extract_singletons(g[-1])\n",
    "last_extraction = describe_nodes(g[-1])\n",
    "last_extraction['extraction_round'] = -1\n",
    "extracted_seqs = pd.concat(_extracted_seqs + [last_extraction]).reset_index(drop=True)\n",
    "\n",
    "# Test that we haven't lost any length-x-depth\n",
    "assert np.allclose(\n",
    "    np.asarray((depth_matrix(g[0]) * g[0].vp.length.a).sum(1))[[0, 1, 2]],\n",
    "    extracted_seqs[[0, 1, 2]].multiply(extracted_seqs.length, axis=0).sum(0).values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_list = ['a', 'b', 'c']\n",
    "query_list = extracted_seqs.sort_values('length', ascending=False).head(100).index\n",
    "\n",
    "out = []\n",
    "for query_idx in query_list:\n",
    "    out.append([])\n",
    "    for ref_idx in ref_list:\n",
    "        ref = paths[ref_idx]\n",
    "        query = extracted_seqs.sequence[query_idx]\n",
    "        out[-1].append(edit_ratio(ref, query))\n",
    "out = pd.DataFrame(out, index=query_list, columns=ref_list)\n",
    "\n",
    "out.join(extracted_seqs).sort_values('length', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dotplot(extracted_seqs.sequence[48966], paths['b'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}